"""
æ™ºèƒ½å‹ç¼©ä¼ è¾“ä¼˜åŒ–æ¨¡å— - ç¬¬ä¸‰é˜¶æ®µ
æ”¯æŒåŸºäºæ–‡ä»¶ç±»å‹çš„æ™ºèƒ½å‹ç¼©ç­–ç•¥ã€gzipä¼ è¾“ä¼˜åŒ–ã€æµå¼å¤„ç†å’Œæ€§èƒ½ç›‘æ§
"""
import asyncio
import gzip
import zlib
import aiofiles
import time
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple, Callable, Union
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from collections import defaultdict

from .models import FileItem


class CompressionType(Enum):
    """æ”¯æŒçš„å‹ç¼©ç±»å‹"""
    NONE = "none"
    GZIP = "gzip"
    DEFLATE = "deflate"
    BROTLI = "br"
    AUTO = "auto"


class FileCategory(Enum):
    """æ–‡ä»¶åˆ†ç±»"""
    JSON_SMALL = "json_small"      # <100KB JSON
    JSON_LARGE = "json_large"      # >=100KB JSON
    PNG_SMALL = "png_small"        # <500KB PNG
    PNG_MEDIUM = "png_medium"      # 500KB-2MB PNG
    PNG_LARGE = "png_large"        # >2MB PNG
    OTHER = "other"


@dataclass
class CompressionConfig:
    """å‹ç¼©é…ç½®"""
    # JSONæ–‡ä»¶å‹ç¼©é…ç½®
    force_json_compression: bool = True       # å¼ºåˆ¶JSONæ–‡ä»¶å‹ç¼©
    json_compression_threshold: int = 1024    # JSONæ–‡ä»¶å‹ç¼©é˜ˆå€¼(å­—èŠ‚)
    json_preferred_encoding: str = "gzip, br, deflate"
    
    # PNGæ–‡ä»¶ä¼˜åŒ–é…ç½®
    png_stream_threshold: int = 500 * 1024   # 500KBä»¥ä¸ŠPNGä½¿ç”¨æµå¼å¤„ç†
    png_large_file_threshold: int = 2 * 1024 * 1024  # 2MBä»¥ä¸Šä¸ºå¤§PNGæ–‡ä»¶
    enable_png_optimization: bool = True      # å¯ç”¨PNGæµå¼ä¼˜åŒ–
    
    # é€šç”¨å‹ç¼©é…ç½®
    compression_level: int = 6                # gzipå‹ç¼©çº§åˆ«(1-9)
    stream_chunk_size: int = 64 * 1024      # æµå¼å¤„ç†å—å¤§å°
    enable_compression_cache: bool = True     # å¯ç”¨å‹ç¼©ç¼“å­˜
    cache_max_age: int = 3600                # ç¼“å­˜æœ‰æ•ˆæœŸ(ç§’)
    
    # æ€§èƒ½ç›‘æ§é…ç½®
    enable_performance_tracking: bool = True  # å¯ç”¨æ€§èƒ½è·Ÿè¸ª
    track_compression_ratios: bool = True     # è·Ÿè¸ªå‹ç¼©æ¯”
    track_transfer_savings: bool = True       # è·Ÿè¸ªä¼ è¾“èŠ‚çœ


@dataclass
class CompressionStats:
    """å‹ç¼©ç»Ÿè®¡ä¿¡æ¯"""
    files_processed: int = 0
    total_original_size: int = 0
    total_compressed_size: int = 0
    total_transfer_time: float = 0.0
    compression_ratios: Dict[str, List[float]] = field(default_factory=lambda: defaultdict(list))
    transfer_savings: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    method_usage: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    
    @property
    def overall_compression_ratio(self) -> float:
        """æ€»ä½“å‹ç¼©æ¯”"""
        if self.total_original_size == 0:
            return 1.0
        return self.total_compressed_size / self.total_original_size
    
    @property
    def overall_savings_mb(self) -> float:
        """æ€»ä½“èŠ‚çœçš„MBæ•°"""
        return (self.total_original_size - self.total_compressed_size) / (1024 * 1024)
    
    @property
    def overall_savings_percent(self) -> float:
        """æ€»ä½“èŠ‚çœç™¾åˆ†æ¯”"""
        if self.total_original_size == 0:
            return 0.0
        return (1 - self.overall_compression_ratio) * 100


class FileTypeAnalyzer:
    """æ–‡ä»¶ç±»å‹åˆ†æå™¨"""
    
    @staticmethod
    def categorize_file(file_item: FileItem) -> FileCategory:
        """æ–‡ä»¶åˆ†ç±»"""
        filename = file_item.filename.lower()
        size = getattr(file_item, 'size', 0) or 0
        
        if filename.endswith('.json'):
            if size < 100 * 1024:  # 100KB
                return FileCategory.JSON_SMALL
            else:
                return FileCategory.JSON_LARGE
        elif filename.endswith('.png'):
            if size < 500 * 1024:  # 500KB
                return FileCategory.PNG_SMALL
            elif size < 2 * 1024 * 1024:  # 2MB
                return FileCategory.PNG_MEDIUM
            else:
                return FileCategory.PNG_LARGE
        else:
            return FileCategory.OTHER
    
    @staticmethod
    def should_compress(file_item: FileItem, config: CompressionConfig) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å‹ç¼©"""
        category = FileTypeAnalyzer.categorize_file(file_item)
        
        # JSONæ–‡ä»¶ï¼šå§‹ç»ˆå°è¯•å‹ç¼©
        if category in [FileCategory.JSON_SMALL, FileCategory.JSON_LARGE]:
            return config.force_json_compression
        
        # PNGæ–‡ä»¶ï¼šé€šå¸¸ä¸å‹ç¼©ï¼Œä½†å¯ä»¥ä¼˜åŒ–ä¼ è¾“
        if category in [FileCategory.PNG_SMALL, FileCategory.PNG_MEDIUM, FileCategory.PNG_LARGE]:
            return False  # PNGå·²ç»æ˜¯å‹ç¼©æ ¼å¼
        
        return False
    
    @staticmethod
    def get_optimal_headers(file_item: FileItem, config: CompressionConfig) -> Dict[str, str]:
        """è·å–æœ€ä¼˜è¯·æ±‚å¤´"""
        headers = {}
        category = FileTypeAnalyzer.categorize_file(file_item)
        
        if category in [FileCategory.JSON_SMALL, FileCategory.JSON_LARGE]:
            # JSONæ–‡ä»¶ï¼šå¼ºåˆ¶è¯·æ±‚å‹ç¼©
            headers['Accept-Encoding'] = config.json_preferred_encoding
            # æ˜ç¡®å‘ŠçŸ¥æœåŠ¡å™¨æˆ‘ä»¬æ¥å—å‹ç¼©å†…å®¹
            headers['Accept'] = 'application/json'
        elif category in [FileCategory.PNG_SMALL, FileCategory.PNG_MEDIUM, FileCategory.PNG_LARGE]:
            # PNGæ–‡ä»¶ï¼šä¼˜åŒ–ä¼ è¾“ä½†ä¸å¼ºåˆ¶å‹ç¼©
            headers['Accept'] = 'image/png'
            # å¯¹äºå¤§PNGæ–‡ä»¶ï¼Œå°è¯•è¯·æ±‚Rangeæ”¯æŒ
            if category == FileCategory.PNG_LARGE:
                headers['Range'] = 'bytes=0-'  # æ¢æµ‹Rangeæ”¯æŒ
        
        return headers


class CompressionOptimizer:
    """å‹ç¼©ä¼˜åŒ–å™¨"""
    
    def __init__(self, config: CompressionConfig = None):
        self.config = config or CompressionConfig()
        self.stats = CompressionStats()
        self.cache = {}  # å‹ç¼©èƒ½åŠ›ç¼“å­˜
        
    def optimize_request_headers(self, file_item: FileItem) -> Dict[str, str]:
        """ä¼˜åŒ–è¯·æ±‚å¤´ä»¥è·å¾—æœ€ä½³å‹ç¼©æ•ˆæœ"""
        return FileTypeAnalyzer.get_optimal_headers(file_item, self.config)
    
    async def process_compressed_response(self, response_data: bytes, 
                                        content_encoding: Optional[str],
                                        file_item: FileItem,
                                        progress_callback: Optional[Callable] = None) -> bytes:
        """å¤„ç†å‹ç¼©å“åº”æ•°æ®"""
        
        if not content_encoding:
            # æ— å‹ç¼©ï¼Œç›´æ¥è¿”å›
            return response_data
        
        original_size = len(response_data)
        start_time = time.time()
        
        try:
            # è§£å‹ç¼©æ•°æ®
            if content_encoding.lower() in ['gzip', 'x-gzip']:
                decompressed_data = gzip.decompress(response_data)
                method = "gzip"
            elif content_encoding.lower() == 'deflate':
                decompressed_data = zlib.decompress(response_data)
                method = "deflate"
            elif content_encoding.lower() == 'br':
                try:
                    import brotli
                    decompressed_data = brotli.decompress(response_data)
                    method = "brotli"
                except ImportError:
                    if progress_callback:
                        progress_callback("âš ï¸  Brotliè§£å‹ç¼©ä¸æ”¯æŒï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                    return response_data
            else:
                if progress_callback:
                    progress_callback(f"âš ï¸  ä¸æ”¯æŒçš„å‹ç¼©ç¼–ç : {content_encoding}")
                return response_data
            
            # ç»Ÿè®¡å‹ç¼©æ•ˆæœ
            decompressed_size = len(decompressed_data)
            compression_ratio = original_size / decompressed_size if decompressed_size > 0 else 1.0
            process_time = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._update_compression_stats(file_item, original_size, decompressed_size, 
                                         compression_ratio, method, process_time)
            
            if progress_callback:
                savings_percent = (1 - compression_ratio) * 100
                progress_callback(
                    f"ğŸ“¦ {method.upper()}è§£å‹å®Œæˆ: "
                    f"{original_size/1024:.1f}KB â†’ {decompressed_size/1024:.1f}KB "
                    f"(èŠ‚çœ{savings_percent:.1f}%, è€—æ—¶{process_time*1000:.1f}ms)"
                )
            
            return decompressed_data
            
        except Exception as e:
            if progress_callback:
                progress_callback(f"âŒ è§£å‹ç¼©å¤±è´¥({content_encoding}): {str(e)}")
            # è§£å‹ç¼©å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®
            return response_data
    
    def _update_compression_stats(self, file_item: FileItem, compressed_size: int, 
                                decompressed_size: int, compression_ratio: float,
                                method: str, process_time: float):
        """æ›´æ–°å‹ç¼©ç»Ÿè®¡ä¿¡æ¯"""
        if not self.config.enable_performance_tracking:
            return
        
        category = FileTypeAnalyzer.categorize_file(file_item)
        category_name = category.value
        
        self.stats.files_processed += 1
        self.stats.total_original_size += decompressed_size
        self.stats.total_compressed_size += compressed_size
        self.stats.total_transfer_time += process_time
        
        if self.config.track_compression_ratios:
            self.stats.compression_ratios[category_name].append(compression_ratio)
        
        if self.config.track_transfer_savings:
            savings = decompressed_size - compressed_size
            self.stats.transfer_savings[category_name] += savings
        
        self.stats.method_usage[method] += 1
    
    def get_compression_summary(self) -> Dict[str, Any]:
        """è·å–å‹ç¼©æ•ˆæœæ‘˜è¦"""
        summary = {
            'files_processed': self.stats.files_processed,
            'overall_compression_ratio': self.stats.overall_compression_ratio,
            'overall_savings_mb': self.stats.overall_savings_mb,
            'overall_savings_percent': self.stats.overall_savings_percent,
            'transfer_time_total': self.stats.total_transfer_time,
            'methods_used': dict(self.stats.method_usage)
        }
        
        # æŒ‰æ–‡ä»¶ç±»å‹çš„å‹ç¼©æ•ˆæœ
        category_summary = {}
        for category, ratios in self.stats.compression_ratios.items():
            if ratios:
                avg_ratio = sum(ratios) / len(ratios)
                avg_savings = (1 - avg_ratio) * 100
                total_savings = self.stats.transfer_savings.get(category, 0)
                
                category_summary[category] = {
                    'files': len(ratios),
                    'avg_compression_ratio': avg_ratio,
                    'avg_savings_percent': avg_savings,
                    'total_savings_mb': total_savings / (1024 * 1024)
                }
        
        summary['category_breakdown'] = category_summary
        return summary


class StreamingOptimizer:
    """æµå¼ä¼ è¾“ä¼˜åŒ–å™¨"""
    
    def __init__(self, config: CompressionConfig = None):
        self.config = config or CompressionConfig()
    
    async def optimize_png_streaming(self, response, file_item: FileItem, 
                                   local_path: Path,
                                   progress_callback: Optional[Callable] = None) -> bool:
        """PNGæ–‡ä»¶æµå¼ä¼ è¾“ä¼˜åŒ–"""
        category = FileTypeAnalyzer.categorize_file(file_item)
        
        if category not in [FileCategory.PNG_MEDIUM, FileCategory.PNG_LARGE]:
            # å°PNGæ–‡ä»¶ä½¿ç”¨å¸¸è§„å¤„ç†
            return False
        
        try:
            file_size = getattr(file_item, 'size', 0) or response.content_length or 0
            
            if progress_callback:
                progress_callback(f"ğŸ–¼ï¸  å¯ç”¨PNGæµå¼ä¼˜åŒ–: {file_item.filename} ({file_size/1024/1024:.1f}MB)")
            
            # ä½¿ç”¨ä¼˜åŒ–çš„å—å¤§å°
            if category == FileCategory.PNG_LARGE:
                chunk_size = min(128 * 1024, self.config.stream_chunk_size * 2)  # å¤§æ–‡ä»¶ç”¨æ›´å¤§å—
            else:
                chunk_size = self.config.stream_chunk_size
            
            downloaded = 0
            last_progress_time = time.time()
            start_time = time.time()
            
            async with aiofiles.open(local_path, 'wb') as f:
                async for chunk in response.iter_chunks(chunk_size):
                    await f.write(chunk)
                    downloaded += len(chunk)
                    
                    # ä¼˜åŒ–çš„è¿›åº¦æ›´æ–°é¢‘ç‡
                    current_time = time.time()
                    if current_time - last_progress_time >= 1.0:  # 1ç§’æ›´æ–°ä¸€æ¬¡
                        if file_size > 0:
                            progress = (downloaded / file_size) * 100
                            speed = downloaded / (current_time - start_time) if current_time > start_time else 0
                            speed_mb = speed / (1024 * 1024)
                            
                            if progress_callback:
                                progress_callback(
                                    f"ğŸ–¼ï¸  PNGæµå¼ä¸‹è½½: {progress:.1f}% "
                                    f"({downloaded/1024/1024:.1f}MB/{file_size/1024/1024:.1f}MB) "
                                    f"é€Ÿåº¦:{speed_mb:.1f}MB/s"
                                )
                        
                        last_progress_time = current_time
            
            elapsed_time = time.time() - start_time
            if progress_callback:
                avg_speed = (downloaded / elapsed_time) / (1024 * 1024) if elapsed_time > 0 else 0
                progress_callback(
                    f"âœ… PNGæµå¼ä¸‹è½½å®Œæˆ: {downloaded/1024/1024:.1f}MB "
                    f"(å¹³å‡{avg_speed:.1f}MB/s, å—å¤§å°:{chunk_size/1024:.1f}KB)"
                )
            
            return True
            
        except Exception as e:
            if progress_callback:
                progress_callback(f"âŒ PNGæµå¼ä¼˜åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def should_use_streaming(self, file_item: FileItem) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨æµå¼ä¼ è¾“"""
        category = FileTypeAnalyzer.categorize_file(file_item)
        file_size = getattr(file_item, 'size', 0) or 0
        
        # PNGæ–‡ä»¶è¶…è¿‡é˜ˆå€¼æ—¶ä½¿ç”¨æµå¼ä¼ è¾“
        if category in [FileCategory.PNG_MEDIUM, FileCategory.PNG_LARGE]:
            return file_size >= self.config.png_stream_threshold
        
        # å¤§JSONæ–‡ä»¶ä¹Ÿå¯ä»¥è€ƒè™‘æµå¼ä¼ è¾“
        if category == FileCategory.JSON_LARGE:
            return file_size >= 500 * 1024  # 500KBä»¥ä¸Šçš„JSONæ–‡ä»¶
        
        return False


class CompressionManager:
    """å‹ç¼©ä¼ è¾“ç®¡ç†å™¨ - ç»Ÿä¸€é—¨é¢"""
    
    def __init__(self, config: CompressionConfig = None):
        self.config = config or CompressionConfig()
        self.optimizer = CompressionOptimizer(self.config)
        self.streaming = StreamingOptimizer(self.config)
        self.session_start_time = time.time()
    
    def analyze_file_requirements(self, file_item: FileItem) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶çš„ä¼ è¾“ä¼˜åŒ–éœ€æ±‚"""
        category = FileTypeAnalyzer.categorize_file(file_item)
        
        analysis = {
            'category': category.value,
            'should_compress': FileTypeAnalyzer.should_compress(file_item, self.config),
            'should_stream': self.streaming.should_use_streaming(file_item),
            'optimal_headers': self.optimizer.optimize_request_headers(file_item),
            'estimated_savings': self._estimate_compression_savings(file_item)
        }
        
        return analysis
    
    def _estimate_compression_savings(self, file_item: FileItem) -> Dict[str, Union[float, str]]:
        """ä¼°ç®—å‹ç¼©èŠ‚çœæ•ˆæœ"""
        category = FileTypeAnalyzer.categorize_file(file_item)
        file_size = getattr(file_item, 'size', 0) or 0
        
        if category in [FileCategory.JSON_SMALL, FileCategory.JSON_LARGE]:
            # JSONæ–‡ä»¶é€šå¸¸æœ‰70-80%çš„å‹ç¼©æ¯”
            estimated_ratio = 0.25  # å‹ç¼©åå¤§å°ä¸ºåŸæ¥çš„25%
            estimated_savings = file_size * (1 - estimated_ratio)
            return {
                'compression_ratio': estimated_ratio,
                'estimated_savings_bytes': estimated_savings,
                'estimated_savings_percent': (1 - estimated_ratio) * 100,
                'method': 'gzip'
            }
        elif category in [FileCategory.PNG_SMALL, FileCategory.PNG_MEDIUM, FileCategory.PNG_LARGE]:
            # PNGæ–‡ä»¶å·²ç»å‹ç¼©ï¼Œä¸»è¦ä¼˜åŒ–ä¼ è¾“æ•ˆç‡
            return {
                'compression_ratio': 1.0,
                'estimated_savings_bytes': 0,
                'estimated_savings_percent': 0,
                'method': 'streaming_optimization'
            }
        else:
            return {
                'compression_ratio': 1.0,
                'estimated_savings_bytes': 0,
                'estimated_savings_percent': 0,
                'method': 'none'
            }
    
    async def optimize_download(self, response, file_item: FileItem, 
                              local_path: Path,
                              progress_callback: Optional[Callable] = None) -> bool:
        """ä¼˜åŒ–ä¸‹è½½è¿‡ç¨‹"""
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æµå¼å¤„ç†
        if self.streaming.should_use_streaming(file_item):
            return await self.streaming.optimize_png_streaming(
                response, file_item, local_path, progress_callback
            )
        
        # å¸¸è§„å¤„ç†
        return False
    
    async def process_response_data(self, response_data: bytes,
                                  content_encoding: Optional[str],
                                  file_item: FileItem,
                                  progress_callback: Optional[Callable] = None) -> bytes:
        """å¤„ç†å“åº”æ•°æ®"""
        return await self.optimizer.process_compressed_response(
            response_data, content_encoding, file_item, progress_callback
        )
    
    def get_session_summary(self) -> Dict[str, Any]:
        """è·å–ä¼šè¯æ‘˜è¦"""
        session_time = time.time() - self.session_start_time
        compression_summary = self.optimizer.get_compression_summary()
        
        summary = {
            'session_duration': session_time,
            'compression_stats': compression_summary,
            'optimization_enabled': {
                'json_compression': self.config.force_json_compression,
                'png_streaming': self.config.enable_png_optimization,
                'performance_tracking': self.config.enable_performance_tracking
            }
        }
        
        return summary
    
    def reset_session_stats(self):
        """é‡ç½®ä¼šè¯ç»Ÿè®¡"""
        self.optimizer.stats = CompressionStats()
        self.session_start_time = time.time() 