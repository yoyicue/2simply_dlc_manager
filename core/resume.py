"""
æ™ºèƒ½æ–­ç‚¹ç»­ä¼ æ¨¡å— - Day 2ç”¨æˆ·ä½“éªŒä¼˜åŒ–ç‰ˆæœ¬
æ”¯æŒå¤§æ–‡ä»¶æ–­ç‚¹ç»­ä¼ ã€ç½‘ç»œä¸­æ–­æ¢å¤ã€å®Œæ•´æ€§æ ¡éªŒã€æ€§èƒ½ç›‘æ§
"""
import asyncio
import aiofiles
import hashlib
import time
import json
from pathlib import Path
from typing import Optional, Dict, Any, Tuple, Callable, Union, List
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict
from enum import Enum

from .models import FileItem, DownloadStatus
from .network import AsyncHttpClient


class HashAlgorithm(Enum):
    """æ”¯æŒçš„å“ˆå¸Œç®—æ³•"""
    MD5 = "md5"
    SHA1 = "sha1"
    SHA256 = "sha256"
    SHA512 = "sha512"
    CRC32 = "crc32"


@dataclass
class IntegrityResult:
    """å®Œæ•´æ€§æ ¡éªŒç»“æœ"""
    is_valid: bool
    algorithm: HashAlgorithm
    expected_hash: str
    calculated_hash: str
    file_size: int
    verification_time: float
    error_message: Optional[str] = None
    
    @property
    def hash_match(self) -> bool:
        """å“ˆå¸Œå€¼æ˜¯å¦åŒ¹é…"""
        return self.expected_hash.lower() == self.calculated_hash.lower()
    
    @property
    def summary(self) -> str:
        """è·å–æ ¡éªŒç»“æœæ‘˜è¦"""
        if self.is_valid:
            return f"âœ… {self.algorithm.value.upper()}æ ¡éªŒé€šè¿‡ (è€—æ—¶{self.verification_time:.1f}s)"
        else:
            return f"âŒ {self.algorithm.value.upper()}ä¸åŒ¹é…: æœŸæœ›{self.expected_hash[:8]}...ï¼Œå®é™…{self.calculated_hash[:8]}..."


@dataclass
class IntegrityCache:
    """å®Œæ•´æ€§æ ¡éªŒç¼“å­˜"""
    file_path: str
    file_size: int
    mtime: float
    algorithm: HashAlgorithm
    hash_value: str
    verification_time: float
    cache_timestamp: str
    
    def is_valid(self, current_path: Path) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆ"""
        try:
            if not current_path.exists():
                return False
            
            stat_info = current_path.stat()
            return (stat_info.st_size == self.file_size and 
                   stat_info.st_mtime == self.mtime)
        except:
            return False


@dataclass
class ProgressInfo:
    """è¯¦ç»†è¿›åº¦ä¿¡æ¯ - Day 2 æ–°å¢"""
    current_bytes: int = 0
    total_bytes: Optional[int] = None
    speed_bps: float = 0.0  # å­—èŠ‚/ç§’
    eta_seconds: Optional[float] = None
    start_time: float = field(default_factory=time.time)
    last_update_time: float = field(default_factory=time.time)
    
    @property
    def progress_percent(self) -> float:
        if not self.total_bytes or self.total_bytes == 0:
            return 0.0
        return min((self.current_bytes / self.total_bytes) * 100, 100.0)
    
    @property
    def elapsed_time(self) -> float:
        return time.time() - self.start_time
    
    def update_speed(self, new_bytes: int):
        """æ›´æ–°ä¸‹è½½é€Ÿåº¦è®¡ç®—"""
        current_time = time.time()
        time_diff = current_time - self.last_update_time
        
        if time_diff > 0:
            byte_diff = new_bytes - self.current_bytes
            self.speed_bps = byte_diff / time_diff
            
            # è®¡ç®—é¢„è®¡å‰©ä½™æ—¶é—´
            if self.total_bytes and self.speed_bps > 0:
                remaining_bytes = self.total_bytes - new_bytes
                self.eta_seconds = remaining_bytes / self.speed_bps
        
        self.current_bytes = new_bytes
        self.last_update_time = current_time


@dataclass
class ResumeConfig:
    """æ–­ç‚¹ç»­ä¼ é…ç½®"""
    min_resume_size: int = 2 * 1024 * 1024  # 2MB
    chunk_size: int = 64 * 1024  # 64KB
    max_retries: int = 5
    retry_delay: float = 1.0
    enable_integrity_cache: bool = True
    integrity_cache_max_age: int = 24  # å°æ—¶
    supported_algorithms: List[HashAlgorithm] = field(default_factory=lambda: [
        HashAlgorithm.MD5, HashAlgorithm.SHA1, HashAlgorithm.SHA256
    ])


class IntegrityManager:
    """å®Œæ•´æ€§æ ¡éªŒç®¡ç†å™¨ - Day 2 å¢å¼ºç‰ˆæœ¬"""
    
    def __init__(self, config: ResumeConfig, cache_dir: Optional[Path] = None):
        self.config = config
        self.cache_dir = cache_dir or Path.home() / ".dlc_manager" / "integrity_cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_file = self.cache_dir / "integrity_cache.json"
        self._cache: Dict[str, IntegrityCache] = {}
        self._load_cache()
    
    def _load_cache(self):
        """åŠ è½½å®Œæ•´æ€§æ ¡éªŒç¼“å­˜"""
        if not self.config.enable_integrity_cache or not self.cache_file.exists():
            return
        
        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            for key, data in cache_data.items():
                self._cache[key] = IntegrityCache(
                    file_path=data['file_path'],
                    file_size=data['file_size'],
                    mtime=data['mtime'],
                    algorithm=HashAlgorithm(data['algorithm']),
                    hash_value=data['hash_value'],
                    verification_time=data['verification_time'],
                    cache_timestamp=data['cache_timestamp']
                )
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å®Œæ•´æ€§ç¼“å­˜å¤±è´¥: {e}")
            self._cache = {}
    
    def _save_cache(self):
        """ä¿å­˜å®Œæ•´æ€§æ ¡éªŒç¼“å­˜"""
        if not self.config.enable_integrity_cache:
            return
        
        try:
            cache_data = {}
            for key, cache_item in self._cache.items():
                cache_data[key] = {
                    'file_path': cache_item.file_path,
                    'file_size': cache_item.file_size,
                    'mtime': cache_item.mtime,
                    'algorithm': cache_item.algorithm.value,
                    'hash_value': cache_item.hash_value,
                    'verification_time': cache_item.verification_time,
                    'cache_timestamp': cache_item.cache_timestamp
                }
            
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å®Œæ•´æ€§ç¼“å­˜å¤±è´¥: {e}")
    
    def _get_cache_key(self, file_path: Path, algorithm: HashAlgorithm) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{file_path.absolute()}:{algorithm.value}"
    
    def _detect_algorithm(self, hash_value: str) -> HashAlgorithm:
        """æ ¹æ®å“ˆå¸Œå€¼é•¿åº¦æ£€æµ‹ç®—æ³•"""
        hash_length = len(hash_value)
        if hash_length == 32:
            return HashAlgorithm.MD5
        elif hash_length == 40:
            return HashAlgorithm.SHA1
        elif hash_length == 64:
            return HashAlgorithm.SHA256
        elif hash_length == 128:
            return HashAlgorithm.SHA512
        elif hash_length == 8:
            return HashAlgorithm.CRC32
        else:
            # é»˜è®¤ä½¿ç”¨MD5
            return HashAlgorithm.MD5
    
    def calculate_hash_with_progress(self, file_path: Path, algorithm: HashAlgorithm,
                                   progress_callback: Optional[Callable] = None) -> str:
        """å¸¦è¿›åº¦çš„å“ˆå¸Œè®¡ç®— - æ”¯æŒå¤šç§ç®—æ³•"""
        # åˆ›å»ºå“ˆå¸Œå¯¹è±¡
        if algorithm == HashAlgorithm.MD5:
            hasher = hashlib.md5()
        elif algorithm == HashAlgorithm.SHA1:
            hasher = hashlib.sha1()
        elif algorithm == HashAlgorithm.SHA256:
            hasher = hashlib.sha256()
        elif algorithm == HashAlgorithm.SHA512:
            hasher = hashlib.sha512()
        elif algorithm == HashAlgorithm.CRC32:
            import zlib
            crc_value = 0
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å“ˆå¸Œç®—æ³•: {algorithm}")
        
        file_size = file_path.stat().st_size
        processed = 0
        start_time = time.time()
        last_update = start_time
        
        with open(file_path, 'rb') as f:
            while chunk := f.read(self.config.chunk_size):
                if algorithm == HashAlgorithm.CRC32:
                    crc_value = zlib.crc32(chunk, crc_value)
                else:
                    hasher.update(chunk)
                
                processed += len(chunk)
                current_time = time.time()
                
                # æ¯500msæˆ–æ¯1MBæ›´æ–°ä¸€æ¬¡è¿›åº¦
                if (current_time - last_update >= 0.5 or 
                    processed % (1024*1024) < self.config.chunk_size) and progress_callback and file_size > 0:
                    
                    progress = (processed / file_size) * 100
                    elapsed = current_time - start_time
                    if elapsed > 0:
                        speed_mb = (processed / elapsed) / (1024 * 1024)
                        eta = (file_size - processed) / (processed / elapsed) if processed > 0 else 0
                        progress_callback(
                            f"ğŸ” {algorithm.value.upper()}æ ¡éªŒè¿›åº¦: {progress:.1f}% "
                            f"({processed/1024/1024:.1f}MB/{file_size/1024/1024:.1f}MB) "
                            f"é€Ÿåº¦:{speed_mb:.1f}MB/s å‰©ä½™:{eta:.0f}s"
                        )
                    last_update = current_time
        
        if algorithm == HashAlgorithm.CRC32:
            return f"{crc_value & 0xffffffff:08x}"
        else:
            return hasher.hexdigest()
    
    def verify_integrity_enhanced(self, file_item: FileItem, local_path: Path,
                                progress_callback: Optional[Callable] = None) -> IntegrityResult:
        """å¢å¼ºçš„å®Œæ•´æ€§æ ¡éªŒ - æ”¯æŒå¤šç®—æ³•ã€ç¼“å­˜ã€å¢é‡æ ¡éªŒ"""
        if not local_path.exists():
            return IntegrityResult(
                is_valid=False,
                algorithm=HashAlgorithm.MD5,
                expected_hash="",
                calculated_hash="",
                file_size=0,
                verification_time=0.0,
                error_message="æ–‡ä»¶ä¸å­˜åœ¨"
            )
        
        file_size = local_path.stat().st_size
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if file_item.size and file_size != file_item.size:
            return IntegrityResult(
                is_valid=False,
                algorithm=HashAlgorithm.MD5,
                expected_hash=file_item.md5 or "",
                calculated_hash="",
                file_size=file_size,
                verification_time=0.0,
                error_message=f"æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›{file_item.size}å­—èŠ‚ï¼Œå®é™…{file_size}å­—èŠ‚"
            )
        
        # æ£€æŸ¥å“ˆå¸Œå€¼
        if not file_item.md5:
            return IntegrityResult(
                is_valid=True,
                algorithm=HashAlgorithm.MD5,
                expected_hash="",
                calculated_hash="",
                file_size=file_size,
                verification_time=0.0,
                error_message="æ— å“ˆå¸Œä¿¡æ¯ï¼Œä»…éªŒè¯æ–‡ä»¶å¤§å°"
            )
        
        # æ£€æµ‹å“ˆå¸Œç®—æ³•
        algorithm = self._detect_algorithm(file_item.md5)
        cache_key = self._get_cache_key(local_path, algorithm)
        
        # æ£€æŸ¥ç¼“å­˜
        if self.config.enable_integrity_cache and cache_key in self._cache:
            cached_result = self._cache[cache_key]
            if cached_result.is_valid(local_path):
                if progress_callback:
                    progress_callback(f"ğŸš€ ä½¿ç”¨ç¼“å­˜çš„{algorithm.value.upper()}æ ¡éªŒç»“æœ")
                
                return IntegrityResult(
                    is_valid=cached_result.hash_value.lower() == file_item.md5.lower(),
                    algorithm=algorithm,
                    expected_hash=file_item.md5,
                    calculated_hash=cached_result.hash_value,
                    file_size=file_size,
                    verification_time=0.0  # ç¼“å­˜å‘½ä¸­ï¼Œæ—¶é—´ä¸º0
                )
        
        # æ‰§è¡Œå®é™…æ ¡éªŒ
        if progress_callback:
            progress_callback(f"ğŸ” å¼€å§‹{algorithm.value.upper()}å®Œæ•´æ€§æ ¡éªŒ ({file_size/1024/1024:.1f}MB)...")
        
        try:
            start_time = time.time()
            calculated_hash = self.calculate_hash_with_progress(local_path, algorithm, progress_callback)
            verification_time = time.time() - start_time
            
            is_valid = calculated_hash.lower() == file_item.md5.lower()
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if self.config.enable_integrity_cache:
                stat_info = local_path.stat()
                self._cache[cache_key] = IntegrityCache(
                    file_path=str(local_path.absolute()),
                    file_size=file_size,
                    mtime=stat_info.st_mtime,
                    algorithm=algorithm,
                    hash_value=calculated_hash,
                    verification_time=verification_time,
                    cache_timestamp=datetime.now().isoformat()
                )
                self._save_cache()
            
            result = IntegrityResult(
                is_valid=is_valid,
                algorithm=algorithm,
                expected_hash=file_item.md5,
                calculated_hash=calculated_hash,
                file_size=file_size,
                verification_time=verification_time
            )
            
            if progress_callback:
                progress_callback(result.summary)
            
            return result
            
        except Exception as e:
            error_msg = f"{algorithm.value.upper()}è®¡ç®—å¤±è´¥: {str(e)}"
            if progress_callback:
                progress_callback(f"âŒ {error_msg}")
            
            return IntegrityResult(
                is_valid=False,
                algorithm=algorithm,
                expected_hash=file_item.md5,
                calculated_hash="",
                file_size=file_size,
                verification_time=0.0,
                error_message=error_msg
            )
    
    def batch_verify_integrity(self, file_items: List[Tuple[FileItem, Path]],
                             progress_callback: Optional[Callable] = None) -> List[IntegrityResult]:
        """æ‰¹é‡å®Œæ•´æ€§æ ¡éªŒ - ä¼˜åŒ–æ€§èƒ½"""
        results = []
        total_files = len(file_items)
        
        for idx, (file_item, local_path) in enumerate(file_items):
            if progress_callback:
                progress_callback(f"ğŸ“‹ æ‰¹é‡æ ¡éªŒè¿›åº¦: {idx+1}/{total_files}")
            
            result = self.verify_integrity_enhanced(file_item, local_path, progress_callback)
            results.append(result)
        
        return results
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.config.enable_integrity_cache:
            return {"enabled": False}
        
        total_entries = len(self._cache)
        algorithms_count = defaultdict(int)
        total_verification_time = 0.0
        
        for cache_item in self._cache.values():
            algorithms_count[cache_item.algorithm.value] += 1
            total_verification_time += cache_item.verification_time
        
        return {
            "enabled": True,
            "total_entries": total_entries,
            "algorithms_distribution": dict(algorithms_count),
            "total_saved_time": total_verification_time,
            "cache_file_size": self.cache_file.stat().st_size if self.cache_file.exists() else 0
        }
    
    def cleanup_cache(self, max_age_hours: int = None):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        if not self.config.enable_integrity_cache:
            return
        
        max_age = max_age_hours or self.config.integrity_cache_max_age
        cutoff_time = datetime.now() - timedelta(hours=max_age)
        
        cleaned_count = 0
        for key in list(self._cache.keys()):
            cache_item = self._cache[key]
            try:
                cache_time = datetime.fromisoformat(cache_item.cache_timestamp)
                if cache_time < cutoff_time:
                    del self._cache[key]
                    cleaned_count += 1
            except:
                # æ— æ•ˆçš„æ—¶é—´æˆ³ï¼Œåˆ é™¤
                del self._cache[key]
                cleaned_count += 1
        
        if cleaned_count > 0:
            self._save_cache()
            print(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸç¼“å­˜æ¡ç›®")


class ResumeManager:
    """æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨ - Day 2 å¢å¼ºç‰ˆæœ¬"""
    
    def __init__(self, config: ResumeConfig = None):
        self.config = config or ResumeConfig()
        self.integrity_manager = IntegrityManager(self.config)
        self.session_stats = {
            'resume_attempts': 0,
            'resume_successes': 0,
            'bytes_resumed': 0,
            'time_saved': 0.0
        }
    
    async def probe_resume_support(self, client: AsyncHttpClient, url: str) -> Dict[str, Any]:
        """æ¢æµ‹æœåŠ¡å™¨æ–­ç‚¹ç»­ä¼ æ”¯æŒ - Day 2 æ™ºèƒ½ç¼“å­˜ç‰ˆæœ¬"""
        
        # 1. æ£€æŸ¥ç¼“å­˜
        domain = url.split('/')[2] if url.startswith('http') else url.split('/')[0]
        if domain in self.server_capabilities:
            cached = self.server_capabilities[domain]
            if time.time() - cached['cached_at'] < 3600:  # 1å°æ—¶ç¼“å­˜
                return cached['info']
        
        try:
            # 2. å…ˆå°è¯•HEADè¯·æ±‚
            response_info = await client.head_request(url)
            
            basic_info = {
                'supports_range': response_info.get('accept_ranges', False),
                'content_length': response_info.get('content_length'),
                'etag': response_info.get('etag'),
                'last_modified': response_info.headers.get('last-modified'),
                'status_code': response_info['status_code'],
                'server_type': response_info.headers.get('server', 'unknown')
            }
            
            # 3. å¦‚æœå£°ç§°æ”¯æŒRangeï¼Œè¿›è¡Œå®é™…æµ‹è¯•
            if basic_info['supports_range'] and basic_info['content_length']:
                content_length = int(basic_info['content_length'])
                if content_length > 1024:  # åªå¯¹å¤§äº1KBçš„æ–‡ä»¶æµ‹è¯•
                    # è¯·æ±‚å‰512å­—èŠ‚è¿›è¡ŒRangeæµ‹è¯•
                    range_test = await self._test_range_request(client, url, 0, 511)
                    basic_info['range_test_passed'] = range_test
                    basic_info['actually_supports_range'] = range_test
                else:
                    basic_info['range_test_passed'] = True
                    basic_info['actually_supports_range'] = True
            else:
                basic_info['range_test_passed'] = False
                basic_info['actually_supports_range'] = False
            
            # 4. ç¼“å­˜ç»“æœ
            self.server_capabilities[domain] = {
                'info': basic_info,
                'cached_at': time.time()
            }
            
            return basic_info
            
        except Exception as e:
            return {
                'supports_range': False,
                'actually_supports_range': False,
                'range_test_passed': False,
                'error': str(e),
                'status_code': 0
            }
    
    async def _test_range_request(self, client: AsyncHttpClient, url: str, 
                                start: int, end: int) -> bool:
        """å®é™…æµ‹è¯•Rangeè¯·æ±‚ - Day 2 æ–°å¢"""
        try:
            headers = {'Range': f'bytes={start}-{end}'}
            async with client.stream_download(url, headers) as response:
                if response.status_code == 206:  # Partial Content
                    # è¯»å–ä¸€äº›æ•°æ®ç¡®è®¤Rangeå·¥ä½œæ­£å¸¸
                    chunk_count = 0
                    async for chunk in response.iter_chunks():
                        chunk_count += 1
                        if chunk_count >= 3:  # è¯»å–å‡ ä¸ªchunkå°±å¤Ÿäº†
                            break
                    return True
                return False
        except Exception:
            return False
    
    def should_resume(self, file_item: FileItem, local_path: Path) -> Tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ–­ç‚¹ç»­ä¼  - O(1)åˆ¤æ–­ï¼Œè¿”å›åŸå› """
        # åŸºæœ¬æ¡ä»¶æ£€æŸ¥
        if not local_path.exists():
            return False, "æ–‡ä»¶ä¸å­˜åœ¨"
        
        local_size = local_path.stat().st_size
        
        # æ–‡ä»¶å¤ªå°ä¸å€¼å¾—ç»­ä¼ 
        if local_size < self.config.min_resume_size:
            return False, f"æ–‡ä»¶è¿‡å°({local_size/1024:.1f}KB < {self.config.min_resume_size/1024:.1f}KB)"
        
        # å¦‚æœçŸ¥é“æ€»å¤§å°ï¼Œæ£€æŸ¥æ˜¯å¦å®Œæ•´
        if file_item.size and local_size >= file_item.size:
            return False, "æ–‡ä»¶å·²å®Œæ•´"
        
        return True, f"å¯ç»­ä¼ ({local_size/1024/1024:.1f}MBå·²ä¸‹è½½)"
    
    async def resume_download(self, client: AsyncHttpClient, file_item: FileItem, 
                            url: str, local_path: Path, 
                            progress_callback: Optional[Callable] = None) -> bool:
        """æ‰§è¡Œæ–­ç‚¹ç»­ä¼ ä¸‹è½½ - Day 2 å¢å¼ºè¿›åº¦ç‰ˆæœ¬"""
        
        # 1. æ£€æŸ¥æœ¬åœ°æ–‡ä»¶çŠ¶æ€
        should_resume, reason = self.should_resume(file_item, local_path)
        if not should_resume:
            if progress_callback:
                progress_callback(f"âš ï¸ æ— æ³•ç»­ä¼ : {reason}")
            return False
        
        local_size = local_path.stat().st_size
        self.session_stats['resume_attempts'] += 1
        
        # 2. æ„å»ºRangeè¯·æ±‚å¤´
        headers = {
            'Range': f'bytes={local_size}-'
        }
        
        # 3. å‘èµ·Rangeè¯·æ±‚
        try:
            start_time = time.time()
            progress_info = ProgressInfo(current_bytes=local_size, total_bytes=file_item.size)
            
            if progress_callback:
                progress_callback(f"ğŸ”„ ç»­ä¼ å¼€å§‹: ä»{local_size/1024/1024:.1f}MBå¤„ç»§ç»­...")
            
            async with client.stream_download(url, headers) as response:
                # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦æ”¯æŒRange
                if response.status_code not in (206, 416):  # 206=Partial Content, 416=Range Not Satisfiable
                    if progress_callback:
                        progress_callback(f"âš ï¸ æœåŠ¡å™¨è¿”å›çŠ¶æ€ç {response.status_code}ï¼Œä¸æ”¯æŒç»­ä¼ ")
                    return False
                
                if response.status_code == 416:
                    # æ–‡ä»¶å·²å®Œæ•´ï¼Œæ— éœ€ç»­ä¼ 
                    if progress_callback:
                        progress_callback("âœ… æ–‡ä»¶å·²å®Œæ•´ï¼Œæ— éœ€ç»­ä¼ ")
                    self.session_stats['resume_successes'] += 1
                    return True
                
                # 4. ç»­å†™æ–‡ä»¶
                async with aiofiles.open(local_path, 'ab') as f:  # append binary
                    resumed_bytes = 0  # æœ¬æ¬¡ç»­ä¼ çš„å­—èŠ‚æ•°
                    chunk_count = 0
                    last_progress_update = time.time()
                    
                    async for chunk in response.iter_chunks():
                        await f.write(chunk)
                        resumed_bytes += len(chunk)
                        chunk_count += 1
                        
                        # ä¼˜åŒ–çš„è¿›åº¦æ›´æ–° - é¿å…é¢‘ç¹æ›´æ–°
                        current_time = time.time()
                        if current_time - last_progress_update >= 0.5:  # 500msæ›´æ–°ä¸€æ¬¡
                            current_size = local_size + resumed_bytes
                            progress_info.update_speed(current_size)
                            
                            if progress_callback:
                                speed_mb = progress_info.speed_bps / (1024 * 1024)
                                if progress_info.eta_seconds:
                                    eta_str = f"ï¼Œé¢„è®¡{progress_info.eta_seconds:.0f}ç§’å®Œæˆ"
                                else:
                                    eta_str = ""
                                    
                                progress_callback(
                                    f"ğŸ“¥ ç»­ä¼ ä¸­: {progress_info.progress_percent:.1f}% "
                                    f"({speed_mb:.1f}MB/s{eta_str})"
                                )
                            
                            # æ›´æ–°FileItemè¿›åº¦
                            if file_item.size:
                                file_item.progress = progress_info.progress_percent
                                file_item.downloaded_size = current_size
                            
                            last_progress_update = current_time
                
                # 5. ç»Ÿè®¡ç»­ä¼ æ•ˆæœ
                elapsed_time = time.time() - start_time
                self.session_stats['resume_successes'] += 1
                self.session_stats['bytes_resumed'] += local_size  # èŠ‚çœçš„å­—èŠ‚æ•°
                
                # ä¼°ç®—èŠ‚çœçš„æ—¶é—´ï¼ˆå‡è®¾å¹³å‡é€Ÿåº¦ï¼‰
                if resumed_bytes > 0 and elapsed_time > 0:
                    avg_speed = resumed_bytes / elapsed_time
                    estimated_saved_time = local_size / avg_speed if avg_speed > 0 else 0
                    self.session_stats['time_saved'] += estimated_saved_time
                
                if progress_callback:
                    progress_callback(f"âœ… ç»­ä¼ å®Œæˆï¼èŠ‚çœäº†{local_size/1024/1024:.1f}MBçš„é‡å¤ä¸‹è½½")
                
                return True
                
        except Exception as e:
            # ç»­ä¼ å¤±è´¥ï¼Œå›é€€åˆ°å®Œæ•´ä¸‹è½½
            self.session_stats['resume_successes'] -= 1
            if progress_callback:
                progress_callback(f"âš ï¸ ç»­ä¼ å¤±è´¥: {str(e)}ï¼Œå°†ä½¿ç”¨å®Œæ•´ä¸‹è½½")
            return False
    
    def calculate_md5_with_progress(self, file_path: Path, 
                                  progress_callback: Optional[Callable] = None, 
                                  chunk_size: int = 64*1024) -> str:
        """å¸¦è¿›åº¦çš„MD5è®¡ç®— - Day 2 ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        return self.integrity_manager.calculate_hash_with_progress(
            file_path, HashAlgorithm.MD5, progress_callback
        )
    
    def verify_integrity(self, file_item: FileItem, local_path: Path, 
                        progress_callback: Optional[Callable] = None) -> Tuple[bool, str]:
        """éªŒè¯æ–‡ä»¶å®Œæ•´æ€§ - Day 2 å¢å¼ºç‰ˆæœ¬ï¼Œè¿”å›è¯¦ç»†ç»“æœï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        result = self.integrity_manager.verify_integrity_enhanced(file_item, local_path, progress_callback)
        return result.is_valid, result.summary if result.is_valid else result.error_message or result.summary
    
    def get_stats_summary(self) -> str:
        """è·å–ç»Ÿè®¡æ‘˜è¦ - Day 2 æ–°å¢"""
        return (
            f"æ–­ç‚¹ç»­ä¼ ç»Ÿè®¡: æˆåŠŸç‡{(self.session_stats['resume_successes'] / self.session_stats['resume_attempts']) * 100:.1f}% "
            f"({self.session_stats['resume_successes']}/{self.session_stats['resume_attempts']}) "
            f"èŠ‚çœæµé‡{self.session_stats['bytes_resumed']/1024/1024:.1f}MB "
            f"èŠ‚çœæ—¶é—´{self.session_stats['time_saved']/60:.1f}åˆ†é’Ÿ"
        )


class NetworkRecovery:
    """ç½‘ç»œæ¢å¤æœºåˆ¶ - Day 2 å¢å¼ºç‰ˆæœ¬"""
    
    def __init__(self, max_retries: int = 5, base_delay: float = 1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.error_stats = defaultdict(int)
    
    async def download_with_recovery(self, download_func, progress_callback=None, *args, **kwargs):
        """å¸¦ç½‘ç»œæ¢å¤çš„ä¸‹è½½åŒ…è£…å™¨ - Day 2 å¢å¼ºç‰ˆæœ¬"""
        
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                return await download_func(*args, **kwargs)
            
            except (asyncio.TimeoutError, ConnectionError, OSError) as e:
                last_error = e
                error_type = self.classify_error(e)
                self.error_stats[error_type] += 1
                
                if attempt == self.max_retries - 1:
                    if progress_callback:
                        progress_callback(f"âŒ é‡è¯•{self.max_retries}æ¬¡åä»ç„¶å¤±è´¥: {str(e)}")
                    raise e
                
                # æŒ‡æ•°é€€é¿ç­–ç•¥
                delay = min(self.base_delay * (2 ** attempt), 16.0)
                
                if progress_callback:
                    progress_callback(
                        f"âš ï¸ ç½‘ç»œé”™è¯¯ ({error_type})ï¼Œ{delay:.1f}ç§’åé‡è¯• "
                        f"(ç¬¬{attempt + 1}æ¬¡/å…±{self.max_retries}æ¬¡): {str(e)[:50]}..."
                    )
                
                await asyncio.sleep(delay)
            
            except Exception as e:
                # éç½‘ç»œé”™è¯¯ï¼Œè®°å½•å¹¶ç›´æ¥æŠ›å‡º
                error_type = self.classify_error(e)
                self.error_stats[error_type] += 1
                
                if progress_callback:
                    progress_callback(f"âŒ è‡´å‘½é”™è¯¯ ({error_type}): {str(e)}")
                raise e
        
        # å¦‚æœåˆ°è¿™é‡Œï¼Œè¯´æ˜é‡è¯•æ¬¡æ•°ç”¨å°½
        if progress_callback:
            progress_callback(f"âŒ è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°({self.max_retries})ï¼Œæ”¾å¼ƒä¸‹è½½")
        raise last_error
    
    def classify_error(self, error: Exception) -> str:
        """é”™è¯¯åˆ†ç±» - Day 2 ç²¾å‡†åˆ†æ"""
        error_str = str(error).lower()
        
        if isinstance(error, asyncio.TimeoutError) or 'timeout' in error_str:
            return "timeout"
        elif isinstance(error, ConnectionError) or 'connection' in error_str:
            return "connection"
        elif 'http 5' in error_str or '50' in error_str:
            return "server_error"
        elif 'http 4' in error_str or ('40' in error_str and 'http' in error_str):
            return "client_error"
        elif 'ssl' in error_str or 'certificate' in error_str:
            return "ssl_error"
        elif 'dns' in error_str or 'resolve' in error_str:
            return "dns_error"
        else:
            return "unknown"
    
    def get_error_summary(self) -> str:
        """è·å–é”™è¯¯ç»Ÿè®¡æ‘˜è¦ - Day 2 æ–°å¢"""
        if not self.error_stats:
            return "æ— ç½‘ç»œé”™è¯¯è®°å½•"
        
        total_errors = sum(self.error_stats.values())
        summary_parts = [f"ç½‘ç»œé”™è¯¯ç»Ÿè®¡(å…±{total_errors}æ¬¡)"]
        
        for error_type, count in self.error_stats.items():
            percentage = (count / total_errors) * 100
            summary_parts.append(f"{error_type}:{count}æ¬¡({percentage:.1f}%)")
        
        return " ".join(summary_parts)


# é›†æˆåˆ°ç°æœ‰ä¸‹è½½å™¨çš„ç®€åŒ–æ¥å£
class SmartResume:
    """æ™ºèƒ½æ–­ç‚¹ç»­ä¼ é—¨é¢ç±» - Day 2 å®Œæ•´ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, progress_callback: Optional[Callable] = None):
        self.resume_manager = ResumeManager()
        self.network_recovery = NetworkRecovery()
        self.progress_callback = progress_callback  # è¿›åº¦å›è°ƒå‡½æ•°
        self.session_stats = {
            'total_downloads': 0,
            'successful_downloads': 0,
            'resumed_downloads': 0,
            'bytes_downloaded': 0,
            'start_time': time.time()
        }
    
    async def smart_download(self, client: AsyncHttpClient, file_item: FileItem, 
                           url: str, local_path: Path) -> bool:
        """æ™ºèƒ½ä¸‹è½½ - Day 2 å®Œæ•´ä¼˜åŒ–ç‰ˆæœ¬"""
        
        self.session_stats['total_downloads'] += 1
        download_start = time.time()
        
        try:
            # 1. é¢„æ£€æŸ¥æœåŠ¡å™¨æ”¯æŒ - å¸¦ç¼“å­˜
            if self.progress_callback:
                self.progress_callback("ğŸ” æ£€æŸ¥æœåŠ¡å™¨æ–­ç‚¹ç»­ä¼ æ”¯æŒ...")
                
            server_info = await self.resume_manager.probe_resume_support(client, url)
            server_type = server_info.get('server_type', 'unknown')
            
            # 2. åˆ†æç»­ä¼ å¯è¡Œæ€§
            should_resume, resume_reason = self.resume_manager.should_resume(file_item, local_path)
            server_supports = server_info.get('actually_supports_range', False)
            
            if self.progress_callback:
                self.progress_callback(f"ğŸ“Š æœåŠ¡å™¨:{server_type} Rangeæ”¯æŒ:{server_supports} ç»­ä¼ æ¡ä»¶:{resume_reason}")
            
            # 3. å°è¯•æ–­ç‚¹ç»­ä¼ 
            if should_resume and server_supports:
                local_size = local_path.stat().st_size
                
                resume_success = await self.network_recovery.download_with_recovery(
                    self.resume_manager.resume_download,
                    self.progress_callback,
                    client, file_item, url, local_path, self.progress_callback
                )
                
                if resume_success:
                    # éªŒè¯å®Œæ•´æ€§
                    is_valid, integrity_msg = self.resume_manager.verify_integrity(
                        file_item, local_path, self.progress_callback
                    )
                    
                    if is_valid:
                        self.session_stats['successful_downloads'] += 1
                        self.session_stats['resumed_downloads'] += 1
                        
                        elapsed = time.time() - download_start
                        if self.progress_callback:
                            self.progress_callback(
                                f"âœ… æ–­ç‚¹ç»­ä¼ æˆåŠŸå®Œæˆ (è€—æ—¶{elapsed:.1f}s) "
                                f"- {integrity_msg}"
                            )
                        return True
                    else:
                        # å®Œæ•´æ€§éªŒè¯å¤±è´¥ï¼Œåˆ é™¤é‡ä¸‹
                        if self.progress_callback:
                            self.progress_callback(f"âš ï¸ {integrity_msg}ï¼Œåˆ é™¤æ–‡ä»¶é‡æ–°ä¸‹è½½...")
                        local_path.unlink(missing_ok=True)
            else:
                # ç»™å‡ºå…·ä½“çš„ä¸èƒ½ç»­ä¼ çš„åŸå› 
                if should_resume and not server_supports:
                    reason = ("æœåŠ¡å™¨Rangeæµ‹è¯•å¤±è´¥" if server_info.get('supports_range') 
                             else f"æœåŠ¡å™¨({server_type})ä¸æ”¯æŒæ–­ç‚¹ç»­ä¼ ")
                else:
                    reason = resume_reason
                
                if self.progress_callback:
                    self.progress_callback(f"ğŸ“¥ ä½¿ç”¨å®Œæ•´ä¸‹è½½: {reason}")
            
            # 4. å›é€€åˆ°å®Œæ•´ä¸‹è½½
            download_success = await self.network_recovery.download_with_recovery(
                self._full_download,
                self.progress_callback,
                client, file_item, url, local_path
            )
            
            if download_success:
                self.session_stats['successful_downloads'] += 1
                elapsed = time.time() - download_start
                
                if self.progress_callback:
                    file_size_mb = local_path.stat().st_size / (1024 * 1024)
                    speed_mb = file_size_mb / elapsed if elapsed > 0 else 0
                    self.progress_callback(
                        f"âœ… å®Œæ•´ä¸‹è½½æˆåŠŸ (è€—æ—¶{elapsed:.1f}sï¼Œå¹³å‡{speed_mb:.1f}MB/s)"
                    )
                
                self.session_stats['bytes_downloaded'] += local_path.stat().st_size
                return True
            
            return False
            
        except Exception as e:
            if self.progress_callback:
                self.progress_callback(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")
            return False
    
    async def _full_download(self, client: AsyncHttpClient, file_item: FileItem, 
                           url: str, local_path: Path) -> bool:
        """å®Œæ•´æ–‡ä»¶ä¸‹è½½çš„å†…éƒ¨å®ç° - Day 2 ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # å‡†å¤‡è¯·æ±‚å¤´
            headers = {}
            if file_item.filename.endswith('.json'):
                headers['Accept-Encoding'] = 'gzip, br, deflate'
            
            # æµå¼ä¸‹è½½å®Œæ•´æ–‡ä»¶
            async with client.stream_download(url, headers) as response:
                if response.status_code != 200:
                    raise Exception(f"HTTP {response.status_code}")
                
                # è·å–æ–‡ä»¶å¤§å°
                if response.content_length:
                    file_item.size = response.content_length
                
                # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
                progress_info = ProgressInfo(total_bytes=file_item.size)
                last_progress_update = time.time()
                
                # ä¸‹è½½æ–‡ä»¶
                if file_item.is_binary_file:
                    # äºŒè¿›åˆ¶æ–‡ä»¶
                    async with aiofiles.open(local_path, 'wb') as f:
                        downloaded = 0
                        chunk_count = 0
                        
                        async for chunk in response.iter_chunks():
                            await f.write(chunk)
                            downloaded += len(chunk)
                            chunk_count += 1
                            
                            # ä¼˜åŒ–çš„è¿›åº¦æ›´æ–°
                            current_time = time.time()
                            if (current_time - last_progress_update >= 0.5 or  # 500ms
                                chunk_count % 100 == 0):  # æ¯100ä¸ªchunk
                                
                                progress_info.update_speed(downloaded)
                                
                                if self.progress_callback and file_item.size:
                                    speed_mb = progress_info.speed_bps / (1024 * 1024)
                                    eta_str = f"ï¼Œé¢„è®¡{progress_info.eta_seconds:.0f}ç§’å®Œæˆ" if progress_info.eta_seconds else ""
                                    
                                    self.progress_callback(
                                        f"ğŸ“¥ ä¸‹è½½ä¸­: {progress_info.progress_percent:.1f}% "
                                        f"({speed_mb:.1f}MB/s{eta_str})"
                                    )
                                
                                # æ›´æ–°FileItemè¿›åº¦
                                file_item.progress = progress_info.progress_percent
                                file_item.downloaded_size = downloaded
                                
                                last_progress_update = current_time
                    
                    file_item.progress = 100.0
                
                else:
                    # æ–‡æœ¬æ–‡ä»¶
                    content_bytes = b''
                    async for chunk in response.iter_chunks():
                        content_bytes += chunk
                    
                    try:
                        content = content_bytes.decode('utf-8')
                    except UnicodeDecodeError:
                        content = content_bytes.decode('utf-8', errors='replace')
                    
                    async with aiofiles.open(local_path, 'w', encoding='utf-8') as f:
                        await f.write(content)
                    
                    file_item.progress = 100.0
                
                # éªŒè¯å®Œæ•´æ€§
                is_valid, integrity_msg = self.resume_manager.verify_integrity(
                    file_item, local_path, self.progress_callback
                )
                
                if is_valid:
                    return True
                else:
                    local_path.unlink(missing_ok=True)
                    raise Exception(f"æ–‡ä»¶å®Œæ•´æ€§éªŒè¯å¤±è´¥: {integrity_msg}")
                    
        except Exception as e:
            raise Exception(f"å®Œæ•´ä¸‹è½½å¤±è´¥: {str(e)}")
    
    def get_session_summary(self) -> str:
        """è·å–ä¼šè¯ç»Ÿè®¡æ‘˜è¦ - Day 2 æ–°å¢"""
        elapsed_time = time.time() - self.session_stats['start_time']
        success_rate = (self.session_stats['successful_downloads'] / 
                       self.session_stats['total_downloads'] * 100) if self.session_stats['total_downloads'] > 0 else 0
        
        resume_rate = (self.session_stats['resumed_downloads'] / 
                      self.session_stats['successful_downloads'] * 100) if self.session_stats['successful_downloads'] > 0 else 0
        
        avg_speed = (self.session_stats['bytes_downloaded'] / elapsed_time / (1024 * 1024)) if elapsed_time > 0 else 0
        
        return (
            f"ä¼šè¯ç»Ÿè®¡: æˆåŠŸç‡{success_rate:.1f}% ({self.session_stats['successful_downloads']}/{self.session_stats['total_downloads']}) "
            f"ç»­ä¼ ç‡{resume_rate:.1f}% å¹³å‡é€Ÿåº¦{avg_speed:.1f}MB/s "
            f"æ€»æµé‡{self.session_stats['bytes_downloaded']/1024/1024:.1f}MB è€—æ—¶{elapsed_time/60:.1f}åˆ†é’Ÿ"
        ) 