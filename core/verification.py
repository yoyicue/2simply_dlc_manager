"""
å¹¶è¡ŒMD5éªŒè¯å™¨ - åŸºäºç°æœ‰ThreadPoolExecutoræ¨¡å¼ä¼˜åŒ–
"""
import asyncio
import hashlib
import os
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from PySide6.QtCore import QObject, Signal

from .models import FileItem, DownloadConfig, DownloadStatus, MD5VerifyStatus


@dataclass
class MD5Result:
    """MD5è®¡ç®—ç»“æœ"""
    filename: str
    success: bool
    md5_hash: str
    calculated_md5: str = ""
    file_size: int = 0
    elapsed_time: float = 0.0
    error: str = ""


class ParallelMD5Calculator(QObject):
    """å¹¶è¡ŒMD5éªŒè¯å™¨ - ä¸“ä¸ºæµ·é‡å°æ–‡ä»¶ä¼˜åŒ–"""
    
    # ä¿¡å·å®šä¹‰ - ä¸ç°æœ‰UIå…¼å®¹
    progress_updated = Signal(str, float)  # æ–‡ä»¶å, è¿›åº¦
    file_completed = Signal(str, bool, str)  # æ–‡ä»¶å, æˆåŠŸ, æ¶ˆæ¯  
    overall_progress = Signal(float, int, int)  # è¿›åº¦%, å®Œæˆæ•°, æ€»æ•°
    log_message = Signal(str)  # æ—¥å¿—æ¶ˆæ¯
    
    def __init__(self, config: Optional[DownloadConfig] = None):
        super().__init__()
        self.config = config or DownloadConfig()
        self._cancelled = False
        self._skip_existence_check = False  # æ˜¯å¦è·³è¿‡å­˜åœ¨æ€§æ£€æŸ¥ï¼ˆç”¨äºæ‰¹é‡ä¼˜åŒ–ï¼‰
        
        
    async def calculate_md5_parallel(self, file_items: List[FileItem], 
                                   output_dir: Path) -> Dict[str, MD5Result]:
        """å¹¶è¡Œè®¡ç®—MD5 - æµå¼å¤„ç†ç‰ˆæœ¬ï¼Œä¸ç´¯ç§¯å¤§ç»“æœé›†"""
        if not file_items:
            return {}
        
        self._cancelled = False
        # ğŸš€ æµå¼å¤„ç†ï¼šä¸å†ç»´æŠ¤å¤§çš„resultså­—å…¸
        
        # å¤§æ‰¹é‡æ—¶å¯ç”¨å­˜åœ¨æ€§æ£€æŸ¥è·³è¿‡ä¼˜åŒ–
        self._skip_existence_check = len(file_items) > 100
        if self._skip_existence_check:
            self.log_message.emit(f"ğŸš€ å¤§æ‰¹é‡æ¨¡å¼: è·³è¿‡é‡å¤æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥")
        
        # ä½¿ç”¨ç°æœ‰çš„æ™ºèƒ½å¹¶å‘ç®—æ³•
        optimal_threads = self._get_optimal_threads(file_items)
        batch_size = self._get_optimal_batch_size(file_items)
        
        self.log_message.emit(f"ğŸš€ å¯åŠ¨æµå¼MD5è®¡ç®—: {len(file_items)} ä¸ªæ–‡ä»¶")
        self.log_message.emit(f"âš¡ ä½¿ç”¨ {optimal_threads} çº¿ç¨‹, æ‰¹æ¬¡å¤§å° {batch_size}")
        
        # åˆ†æ‰¹å¤„ç† - å¤ç”¨ç°æœ‰åˆ†æ‰¹é€»è¾‘
        batches = self._create_batches(file_items, batch_size)
        completed_files = 0
        total_files = len(file_items)
        
        # æµå¼å¤„ç†ç»Ÿè®¡
        stream_stats = {
            'success_count': 0,
            'failed_count': 0,
            'total_size': 0,
            'total_time': 0.0
        }
        
        for batch_idx, batch in enumerate(batches):
            if self._cancelled:
                break
                
            self.log_message.emit(f"ğŸ”„ æµå¼å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)} ({len(batch)} ä¸ªæ–‡ä»¶)")
            
            # å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡ - å¤ç”¨ç°æœ‰ThreadPoolExecutoræ¨¡å¼
            batch_results = await self._process_batch_parallel(
                batch, output_dir, optimal_threads
            )
            
            # ğŸš€ æµå¼å¤„ç†æ‰¹æ¬¡ç»“æœï¼šç«‹å³å‘é€ä¿¡å·ï¼Œä¸ç´¯ç§¯
            for result in batch_results:
                completed_files += 1
                
                # æ›´æ–°æµå¼ç»Ÿè®¡
                if result.success:
                    stream_stats['success_count'] += 1
                    stream_stats['total_size'] += result.file_size
                    stream_stats['total_time'] += result.elapsed_time
                else:
                    stream_stats['failed_count'] += 1
                
                # ç«‹å³å‘é€æ–‡ä»¶å®Œæˆä¿¡å·
                success_msg = "" if result.success and not result.error else result.error
                self.file_completed.emit(result.filename, result.success, success_msg)
                
                # æ›´æ–°æ•´ä½“è¿›åº¦
                progress = (completed_files / total_files) * 100
                self.overall_progress.emit(progress, completed_files, total_files)
            
            # ğŸš® æ‰¹æ¬¡ç»“æŸï¼šç«‹å³æ¸…ç†ç»“æœå¹¶åƒåœ¾å›æ”¶
            batch_results.clear()
            del batch_results
            
            # æ‰¹æ¬¡é—´æš‚åœï¼Œä¿æŒUIå“åº”
            await asyncio.sleep(0.01)

            # ğŸš® ä¸»åŠ¨è§¦å‘åƒåœ¾å›æ”¶ï¼Œé¿å…æ‰¹æ¬¡é—´å†…å­˜ç´¯ç§¯
            try:
                import gc
                gc.collect()
            except Exception:
                pass
        
        # è¾“å‡ºæµå¼ç»Ÿè®¡ä¿¡æ¯
        if not self._cancelled:
            self._log_stream_performance_stats(stream_stats)
        
        # ğŸš€ è¿”å›ç©ºå­—å…¸ï¼Œæ‰€æœ‰ç»“æœå·²é€šè¿‡ä¿¡å·æµå¼å‘é€
        return {}
    
    async def _process_batch_parallel(self, batch: List[FileItem], 
                                    output_dir: Path, max_workers: int) -> List[MD5Result]:
        """å¹¶è¡Œå¤„ç†æ‰¹æ¬¡ - å¤ç”¨ç°æœ‰ThreadPoolExecutoræ¨¡å¼"""
        loop = asyncio.get_event_loop()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
            tasks = [
                loop.run_in_executor(executor, self._calculate_single_md5, item, output_dir)
                for item in batch
            ]
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæ”¯æŒå–æ¶ˆ
            batch_results = []
            completed_tasks = 0
            
            for task in asyncio.as_completed(tasks):
                if self._cancelled:
                    # å–æ¶ˆå‰©ä½™ä»»åŠ¡
                    for remaining_task in tasks:
                        if not remaining_task.done():
                            remaining_task.cancel()
                    break
                
                try:
                    result = await task
                    batch_results.append(result)
                    completed_tasks += 1
                except asyncio.CancelledError:
                    break
                except Exception as e:
                    # æ‰¾åˆ°å¯¹åº”çš„æ–‡ä»¶é¡¹
                    corresponding_item = None
                    for item in batch:
                        if not any(r.filename == item.filename for r in batch_results):
                            corresponding_item = item
                            break
                    
                    error_result = MD5Result(
                        filename=corresponding_item.filename if corresponding_item else "unknown",
                        success=False,
                        md5_hash=corresponding_item.md5 if corresponding_item else "",
                        error=f"è®¡ç®—å¼‚å¸¸: {str(e)}"
                    )
                    batch_results.append(error_result)
                    completed_tasks += 1
            
            # ä¸»åŠ¨æ¸…ç†ä»»åŠ¡å¼•ç”¨ï¼Œå¸®åŠ©åƒåœ¾å›æ”¶
            tasks.clear()
            return batch_results
    
    def _calculate_single_md5(self, file_item: FileItem, output_dir: Path) -> MD5Result:
        """è®¡ç®—å•ä¸ªæ–‡ä»¶çš„MD5 - ä¼˜åŒ–ç‰ˆæœ¬"""
        start_time = time.time()
        file_path = output_dir / file_item.full_filename
        
        try:
            # æ™ºèƒ½å­˜åœ¨æ€§æ£€æŸ¥ï¼šå¤§æ‰¹é‡æ—¶è·³è¿‡ï¼ˆå·²åœ¨UIå±‚æ£€æŸ¥ï¼‰ï¼Œå°æ‰¹é‡æ—¶æ£€æŸ¥
            if not self._skip_existence_check:
                if not file_path.exists():
                    return MD5Result(
                        filename=file_item.filename,
                        success=False,
                        md5_hash=file_item.md5,
                        error="æ–‡ä»¶ä¸å­˜åœ¨"
                    )
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            
            # è¯»å–å¹¶è®¡ç®—MD5
            calculated_md5 = self._calculate_file_md5(file_path)
            
            # éªŒè¯MD5
            expected_md5 = file_item.md5.lower()
            actual_md5 = calculated_md5.lower()
            is_match = expected_md5 == actual_md5
            
            elapsed_time = time.time() - start_time
            
            return MD5Result(
                filename=file_item.filename,
                success=True,
                md5_hash=expected_md5,
                calculated_md5=actual_md5,
                file_size=file_size,
                elapsed_time=elapsed_time,
                error="" if is_match else f"MD5ä¸åŒ¹é…: æœŸæœ›{expected_md5}, å®é™…{actual_md5}"
            )
            
        except Exception as e:
            elapsed_time = time.time() - start_time
            return MD5Result(
                filename=file_item.filename,
                success=False,
                md5_hash=file_item.md5,
                elapsed_time=elapsed_time,
                error=f"è®¡ç®—MD5å¤±è´¥: {str(e)}"
            )
    
    def _calculate_file_md5(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶MD5 - Apple Siliconä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # å°è¯•ä½¿ç”¨Apple Siliconä¼˜åŒ–
            return self._calculate_md5_apple_optimized(file_path)
        except Exception as e:
            raise Exception(f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def _calculate_md5_apple_optimized(self, file_path: Path) -> str:
        """Apple Siliconä¼˜åŒ–çš„MD5è®¡ç®— - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        import platform
        
        # æ£€æµ‹Apple Silicon
        is_apple_silicon = (platform.system() == "Darwin" and 
                           platform.machine() == "arm64")
        
        try:
            file_size = file_path.stat().st_size
            
            if is_apple_silicon:
                # Apple Silicon ä¹Ÿæ”¹ä¸ºåˆ†å—è¯»å–ï¼Œé˜²æ­¢å°æ–‡ä»¶ä¸€æ¬¡æ€§è¯»é€ æˆå¤§å†…å­˜ç¢ç‰‡
                if file_size < 256 * 1024:  # <256KB å°æ–‡ä»¶
                    # å°è¯•ç¡¬ä»¶åŠ é€Ÿæ¨¡å¼
                    try:
                        md5_hash = hashlib.md5(usedforsecurity=False)
                    except TypeError:
                        md5_hash = hashlib.md5()
                    
                    with open(file_path, 'rb') as f:
                        while True:
                            if self._cancelled:
                                break
                            chunk = f.read(16384)  # 16KB å—
                            if not chunk:
                                break
                            md5_hash.update(chunk)
                    return md5_hash.hexdigest()
                else:
                    # ä½¿ç”¨Apple Siliconä¼˜åŒ–çš„16KBå—å¤§å°
                    # å°è¯•ç¡¬ä»¶åŠ é€Ÿæ¨¡å¼
                    try:
                        md5_hash = hashlib.md5(usedforsecurity=False)
                    except TypeError:
                        md5_hash = hashlib.md5()
                    
                    with open(file_path, 'rb') as f:
                        while True:
                            if self._cancelled:
                                break
                            chunk = f.read(16384)  # 16KB - Apple Siliconæœ€ä¼˜
                            if not chunk:
                                break
                            md5_hash.update(chunk)
                            # ç«‹å³é‡Šæ”¾chunkå†…å­˜
                            del chunk
                    return md5_hash.hexdigest()
            else:
                # éApple Siliconï¼šä½¿ç”¨åŸæœ‰é€»è¾‘ - å†…å­˜ä¼˜åŒ–
                md5_hash = hashlib.md5()
                
                if file_size < 512 * 1024:  # <512KB (é™ä½é˜ˆå€¼)
                    with open(file_path, 'rb') as f:
                        data = f.read()
                        md5_hash.update(data)
                        del data  # ç«‹å³é‡Šæ”¾
                else:
                    # 64KBå—
                    with open(file_path, 'rb') as f:
                        while True:
                            if self._cancelled:
                                break
                            chunk = f.read(65536)
                            if not chunk:
                                break
                            md5_hash.update(chunk)
                            del chunk  # ç«‹å³é‡Šæ”¾
                
                return md5_hash.hexdigest()
                
        except Exception as e:
            raise Exception(f"MD5è®¡ç®—å¤±è´¥: {str(e)}")
    
    def _get_optimal_threads(self, file_items: List[FileItem]) -> int:
        """è·å–æœ€ä¼˜çº¿ç¨‹æ•° - å¤ç”¨ç°æœ‰ç®—æ³•"""
        # åŸºäºCPUæ ¸å¿ƒæ•°å’Œæ–‡ä»¶ç‰¹å¾
        cpu_cores = os.cpu_count() or 4
        
        # é’ˆå¯¹æµ·é‡å°æ–‡ä»¶ä¼˜åŒ–
        base_threads = min(cpu_cores * 4, 32)  # æœ€å¤š32çº¿ç¨‹
        
        # æ ¹æ®æ–‡ä»¶æ•°é‡è°ƒæ•´
        file_count = len(file_items)
        if file_count < 10:
            return min(file_count, 4)
        elif file_count < 100:
            return min(base_threads // 2, 16)
        else:
            return base_threads
    
    def _get_optimal_batch_size(self, file_items: List[FileItem]) -> int:
        """è·å–æœ€ä¼˜æ‰¹æ¬¡å¤§å° - å†…å­˜å‹å¥½ç‰ˆæœ¬"""
        file_count = len(file_items)
        
        # æ›´æ¿€è¿›çš„å°æ‰¹æ¬¡ç­–ç•¥ï¼Œé˜²æ­¢å†…å­˜ç§¯ç´¯
        if file_count < 20:
            return min(file_count, 8)  # æå°æ•°é‡
        elif file_count < 200:
            return min(25, file_count)  # å°è§„æ¨¡ï¼šå‡å°‘æ‰¹æ¬¡å¤§å°
        elif file_count < 1000:
            return 15  # ä¸­è§„æ¨¡ï¼šæ›´å°æ‰¹æ¬¡
        elif file_count < 5000:
            return 10  # å¤§è§„æ¨¡ï¼šå°æ‰¹æ¬¡é˜²æ­¢å†…å­˜å †ç§¯
        else:
            # æµ·é‡æ–‡ä»¶ï¼šè¶…å°æ‰¹æ¬¡ï¼Œæ¯æ‰¹åªå¤„ç†å°‘é‡æ–‡ä»¶
            return 8
    
    def _create_batches(self, file_items: List[FileItem], batch_size: int) -> List[List[FileItem]]:
        """åˆ›å»ºæ‰¹æ¬¡ - ç®€åŒ–ç‰ˆæœ¬"""
        batches = []
        for i in range(0, len(file_items), batch_size):
            batches.append(file_items[i:i + batch_size])
        return batches
    
    def _log_performance_stats(self, results: Dict[str, MD5Result]):
        """è®°å½•æ€§èƒ½ç»Ÿè®¡ - åŒ…å«å†…å­˜ç›‘æ§"""
        import gc
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ (å¯é€‰)
        memory_mb = 0
        try:
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / 1024 / 1024
        except ImportError:
            # psutilæœªå®‰è£…ï¼Œè·³è¿‡å†…å­˜ç›‘æ§
            pass
        successful_results = [r for r in results.values() if r.success]
        failed_results = [r for r in results.values() if not r.success]
        
        success_count = len(successful_results)
        failed_count = len(failed_results)
        
        self.log_message.emit(f"âœ… å¹¶è¡ŒMD5è®¡ç®—å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        if successful_results:
            total_size = sum(r.file_size for r in successful_results)
            total_time = sum(r.elapsed_time for r in successful_results)
            
            if total_time > 0:
                throughput_mbs = (total_size / 1024 / 1024) / total_time
                files_per_sec = len(successful_results) / total_time
                
                self.log_message.emit(f"ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡: {throughput_mbs:.1f} MB/s, {files_per_sec:.1f} æ–‡ä»¶/ç§’")
                self.log_message.emit(f"ğŸ’¾ å¤„ç†æ•°æ®: {total_size/1024/1024:.1f} MB, è€—æ—¶ {total_time:.1f} ç§’")
                
                # å†…å­˜ç›‘æ§ (å¦‚æœå¯ç”¨)
                if memory_mb > 0:
                    self.log_message.emit(f"ğŸ§  å†…å­˜ä½¿ç”¨: {memory_mb:.1f} MB")
                    
                    # å†…å­˜è­¦å‘Š
                    if memory_mb > 1000:  # è¶…è¿‡1GBè­¦å‘Š
                        self.log_message.emit(f"âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_mb:.1f} MBï¼Œå»ºè®®é‡å¯åº”ç”¨ç¨‹åº")
    
    def _log_stream_performance_stats(self, stream_stats: dict):
        """è®°å½•æµå¼å¤„ç†æ€§èƒ½ç»Ÿè®¡ - å†…å­˜å‹å¥½ç‰ˆæœ¬"""
        import gc
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ (å¯é€‰)
        memory_mb = 0
        try:
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / 1024 / 1024
        except ImportError:
            # psutilæœªå®‰è£…ï¼Œè·³è¿‡å†…å­˜ç›‘æ§
            pass
        
        success_count = stream_stats['success_count']
        failed_count = stream_stats['failed_count']
        total_size = stream_stats['total_size']
        total_time = stream_stats['total_time']
        
        self.log_message.emit(f"âœ… æµå¼MD5è®¡ç®—å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        if success_count > 0 and total_time > 0:
            throughput_mbs = (total_size / 1024 / 1024) / total_time
            files_per_sec = success_count / total_time
            
            self.log_message.emit(f"ğŸ“ˆ æµå¼æ€§èƒ½: {throughput_mbs:.1f} MB/s, {files_per_sec:.1f} æ–‡ä»¶/ç§’")
            self.log_message.emit(f"ğŸ’¾ å¤„ç†æ•°æ®: {total_size/1024/1024:.1f} MB, è€—æ—¶ {total_time:.1f} ç§’")
            
            # å†…å­˜ç›‘æ§ (å¦‚æœå¯ç”¨)
            if memory_mb > 0:
                self.log_message.emit(f"ğŸ§  æµå¼å¤„ç†å†…å­˜: {memory_mb:.1f} MB")
                
                # å†…å­˜æ”¹å–„æç¤º
                if memory_mb < 500:  # å°‘äº500MBæ˜¯å¥½çš„
                    self.log_message.emit(f"âœ… å†…å­˜ä½¿ç”¨è‰¯å¥½: {memory_mb:.1f} MB")
                elif memory_mb > 1000:  # è¶…è¿‡1GBè­¦å‘Š
                    self.log_message.emit(f"âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_mb:.1f} MBï¼Œå»ºè®®é‡å¯åº”ç”¨ç¨‹åº")
    
    def cancel_calculation(self):
        """å–æ¶ˆè®¡ç®—"""
        self._cancelled = True
        self.log_message.emit("âš ï¸ MD5è®¡ç®—å·²å–æ¶ˆ") 