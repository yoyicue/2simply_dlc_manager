"""
å¼‚æ­¥ä¸‹è½½å™¨æ¨¡å— - HTTP/2 ä¼˜åŒ–ç‰ˆæœ¬
"""
import asyncio
import aiofiles
from pathlib import Path
from typing import List, Dict, Optional, Callable, Any
from PySide6.QtCore import QObject, Signal, QModelIndex
import os  # å±€éƒ¨å¯¼å…¥, é¿å…é¡¶å±‚ä¸å¿…è¦ä¾èµ–

from .models import FileItem, DownloadStatus, DownloadConfig
from .network import NetworkManager, AsyncHttpClient, NetworkConfig

# å‘åå…¼å®¹çš„å¯¼å…¥
try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False
    aiohttp = None  # è®¾ç½®ä¸ºNoneä»¥é¿å…NameError


class Downloader(QObject):
    """å¼‚æ­¥ä¸‹è½½å™¨"""
    
    # ä¿¡å·å®šä¹‰
    progress_updated = Signal(str, float)  # æ–‡ä»¶å, è¿›åº¦ç™¾åˆ†æ¯”
    file_completed = Signal(str, bool, str)  # æ–‡ä»¶å, æ˜¯å¦æˆåŠŸ, æ¶ˆæ¯
    overall_progress = Signal(float, int, int)  # æ•´ä½“è¿›åº¦ç™¾åˆ†æ¯”, å·²å®Œæˆæ•°é‡, æ€»æ•°é‡
    check_progress = Signal(float)  # æ–‡ä»¶æ£€æŸ¥è¿›åº¦ç™¾åˆ†æ¯”
    log_message = Signal(str)  # æ—¥å¿—æ¶ˆæ¯
    download_started = Signal()  # ä¸‹è½½å¼€å§‹
    download_finished = Signal(int, int)  # æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡
    download_cancelled = Signal()  # ä¸‹è½½å–æ¶ˆ
    
    def __init__(self, config: Optional[DownloadConfig] = None):
        super().__init__()
        self.config = config or DownloadConfig()
        
        # HTTP/2 ç½‘ç»œå±‚
        self.network_manager = NetworkManager()
        self._http_client: Optional[AsyncHttpClient] = None
        self._network_config: Optional[NetworkConfig] = None
        self._http2_enabled = False
        
        # å‘åå…¼å®¹çš„ä¼šè¯
        self._session: Optional[Any] = None
        
        # ä¸‹è½½çŠ¶æ€
        self._is_cancelled = False
        self._is_downloading = False
        self._semaphore: Optional[asyncio.Semaphore] = None
        
    async def _batch_check_existing_files(self, file_items: List[FileItem], output_dir: Path, data_manager=None) -> tuple[List[FileItem], List[FileItem]]:
        """æ™ºèƒ½æ‰¹é‡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ - é˜¶æ®µä¸€ä¼˜åŒ–ç‰ˆæœ¬
        
        ä½¿ç”¨ç¼“å­˜éªŒè¯ + æ™ºèƒ½å¢é‡æ£€æŸ¥ï¼Œå¤§å¹…æå‡æ€§èƒ½
        
        è¿”å›:
            (existing_files, files_to_download)
        """
        loop = asyncio.get_event_loop()
        
        # ä»DataManagerå¯¼å…¥ç”¨äºç¼“å­˜åˆ†æ
        from .persistence import DataManager
        
        # ä½¿ç”¨ä¼ å…¥çš„data_manageræˆ–åˆ›å»ºæ–°å®ä¾‹
        if data_manager is None:
            data_manager = DataManager()
        
        # -----------------------------
        # 1. é˜¶æ®µäºŒä¼˜å…ˆï¼šBloom Filter O(1)å¿«é€Ÿé¢„è¿‡æ»¤
        # -----------------------------
        bloom_filter = data_manager.bloom_filter
        if bloom_filter and bloom_filter.is_cache_valid():
            self.log_message.emit("âš¡ å¯ç”¨Bloom Filteræœ€ä¼˜å…ˆé¢„è¿‡æ»¤...")
            return await self._optimized_bloom_filter_check(file_items, output_dir, bloom_filter, data_manager)
        
        # -----------------------------
        # 2. é™çº§æ–¹æ¡ˆï¼šåˆ†æç¼“å­˜å¯é æ€§ï¼ˆå½“Bloom Filterä¸å¯ç”¨æ—¶ï¼‰
        # -----------------------------
        self.log_message.emit("ğŸ” åˆ†æç¼“å­˜å¯é æ€§...")
        
        cache_analysis = data_manager.analyze_cache_reliability(file_items, output_dir)
        
        self.log_message.emit(f"ğŸ“Š ç¼“å­˜åˆ†æ: {cache_analysis['reason']}")
        
        # -----------------------------
        # 3. æ ¹æ®ç¼“å­˜å¯é æ€§é€‰æ‹©ç­–ç•¥
        # -----------------------------
        if cache_analysis['recommendation'] == 'cache_reliable':
            return await self._cache_based_check(file_items, output_dir)
        elif cache_analysis['recommendation'] == 'incremental_check':
            return await self._smart_incremental_check(file_items, output_dir, cache_analysis)
        else:
            # å½“ç¼“å­˜ä¸å¯é æˆ–æ–‡ä»¶çŠ¶æ€ä¸ºPENDINGæ—¶ï¼Œæ€»æ˜¯è¿›è¡Œå®Œæ•´ç£ç›˜æ‰«æ
            return await self._optimized_full_scan(file_items, output_dir)
    
    async def _cache_based_check(self, file_items: List[FileItem], output_dir: Path) -> tuple[List[FileItem], List[FileItem]]:
        """åŸºäºç¼“å­˜çš„å¿«é€Ÿæ£€æŸ¥ - å¸¦å…œåº•ç£ç›˜éªŒè¯"""
        self.log_message.emit("âš¡ æ‰§è¡ŒåŸºäºç¼“å­˜çš„å¿«é€Ÿæ£€æŸ¥ï¼ˆå«å…œåº•éªŒè¯ï¼‰...")
        
        existing_files = []
        files_to_download = []
        need_verification = []  # ç¼“å­˜å¯ä¿¡ä½†éœ€è¦å…œåº•éªŒè¯çš„æ–‡ä»¶
        total_files = len(file_items)
        
        # ç¬¬ä¸€è½®ï¼šåŸºäºç¼“å­˜å¿«é€Ÿåˆ†ç±»
        for idx, item in enumerate(file_items):
            if self._is_cancelled:
                break
            
            file_path = output_dir / item.full_filename
            
            # å¦‚æœç¼“å­˜æ ‡è®°ä¸ºå·²éªŒè¯ä¸”æœªè¿‡æœŸï¼ŒåŠ å…¥å…œåº•éªŒè¯é˜Ÿåˆ—
            if (item.status == DownloadStatus.COMPLETED and 
                item.disk_verified and 
                item.is_cache_valid(file_path)):
                need_verification.append(item)
            elif item.status == DownloadStatus.PENDING:
                # PENDINGçŠ¶æ€çš„æ–‡ä»¶éœ€è¦æ£€æŸ¥ç£ç›˜
                need_verification.append(item)
            else:
                files_to_download.append(item)
            
            # å®šæœŸæ›´æ–°è¿›åº¦
            if idx % 500 == 0 or idx == total_files - 1:
                progress_percent = (idx / total_files) * 50  # ç¼“å­˜æ£€æŸ¥å 50%è¿›åº¦
                self.check_progress.emit(progress_percent)
                await asyncio.sleep(0)
        
        # ç¬¬äºŒè½®ï¼šå…œåº•ç£ç›˜éªŒè¯ï¼ˆå³ä½¿ç¼“å­˜å¯ä¿¡ä¹Ÿè¦éªŒè¯ï¼Œé˜²æ­¢æ–‡ä»¶è¢«åˆ é™¤ï¼‰
        if need_verification:
            self.log_message.emit(f"ğŸ” å…œåº•éªŒè¯ {len(need_verification)} ä¸ªç¼“å­˜å¯ä¿¡æ–‡ä»¶...")
            
            verified_existing, verified_missing = await self._parallel_verify_files(
                need_verification, output_dir, progress_offset=50
            )
            
            existing_files.extend(verified_existing)
            files_to_download.extend(verified_missing)
            
            if verified_missing:
                self.log_message.emit(f"âš ï¸  å‘ç° {len(verified_missing)} ä¸ªç¼“å­˜è¿‡æœŸæ–‡ä»¶ï¼ˆæ–‡ä»¶å®é™…ä¸å­˜åœ¨ï¼‰")
        
        self.log_message.emit(f"âœ… ç¼“å­˜+éªŒè¯æ£€æŸ¥å®Œæˆ: {len(existing_files)} ä¸ªæ–‡ä»¶ç¡®è®¤å­˜åœ¨")
        return existing_files, files_to_download
    
    async def _smart_incremental_check(self, file_items: List[FileItem], output_dir: Path, 
                                     cache_analysis: dict) -> tuple[List[FileItem], List[FileItem]]:
        """æ™ºèƒ½å¢é‡æ£€æŸ¥ - ç»“åˆç¼“å­˜ä¸é€‰æ‹©æ€§éªŒè¯"""
        self.log_message.emit("ğŸ§  æ‰§è¡Œæ™ºèƒ½å¢é‡æ£€æŸ¥...")
        
        existing_files = []
        files_to_download = []
        items_need_verification = []
        
        # ç¬¬ä¸€é˜¶æ®µï¼šåŸºäºç¼“å­˜å¿«é€Ÿåˆ†ç±»
        for item in file_items:
            if self._is_cancelled:
                break
            
            file_path = output_dir / item.full_filename
            
            if (item.status == DownloadStatus.COMPLETED and 
                item.disk_verified and 
                item.is_cache_valid(file_path)):
                # ç¼“å­˜å¯ä¿¡ï¼Œç›´æ¥å½’ç±»ä¸ºå­˜åœ¨
                existing_files.append(item)
            elif item.status == DownloadStatus.COMPLETED:
                # éœ€è¦éªŒè¯çš„å·²å®Œæˆæ–‡ä»¶
                items_need_verification.append(item)
            elif item.status == DownloadStatus.PENDING:
                # PENDINGçŠ¶æ€çš„æ–‡ä»¶éœ€è¦æ£€æŸ¥ç£ç›˜
                items_need_verification.append(item)
            else:
                # æ˜ç¡®éœ€è¦ä¸‹è½½çš„æ–‡ä»¶ï¼ˆå¤±è´¥ã€å–æ¶ˆç­‰çŠ¶æ€ï¼‰
                files_to_download.append(item)
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¹¶è¡ŒéªŒè¯éœ€è¦æ£€æŸ¥çš„æ–‡ä»¶
        if items_need_verification:
            self.log_message.emit(f"ğŸ“‹ éªŒè¯ {len(items_need_verification)} ä¸ªå¯ç–‘æ–‡ä»¶...")
            
            verified_existing, verified_missing = await self._parallel_verify_files(
                items_need_verification, output_dir
            )
            
            existing_files.extend(verified_existing)
            files_to_download.extend(verified_missing)
        
        self.log_message.emit(f"âœ… å¢é‡æ£€æŸ¥å®Œæˆ: ä¿¡ä»» {len(existing_files)} ä¸ªï¼Œéœ€éªŒè¯ {len(items_need_verification)} ä¸ª")
        return existing_files, files_to_download
    
    async def _parallel_verify_files(self, file_items: List[FileItem], output_dir: Path, progress_offset: float = 0) -> tuple[List[FileItem], List[FileItem]]:
        """å¹¶è¡ŒéªŒè¯æ–‡ä»¶å­˜åœ¨æ€§å’Œå…ƒæ•°æ®"""
        from concurrent.futures import ThreadPoolExecutor
        import os
        
        def verify_single_file(item: FileItem) -> tuple[FileItem, bool]:
            """éªŒè¯å•ä¸ªæ–‡ä»¶"""
            file_path = output_dir / item.full_filename
            try:
                if not file_path.exists():
                    return item, False
                
                stat_info = file_path.stat()
                
                # æ›´æ–°å…ƒæ•°æ®
                item.update_disk_metadata(file_path)
                item.mark_completed(file_path)
                
                return item, True
            except (OSError, IOError):
                return item, False
        
        loop = asyncio.get_event_loop()
        max_workers = min(8, len(file_items))  # é™åˆ¶å¹¶å‘æ•°
        
        existing_files = []
        files_to_download = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # åˆ†æ‰¹å¤„ç†ä»¥æ§åˆ¶å†…å­˜ä½¿ç”¨
            batch_size = 50
            total_batches = (len(file_items) + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                if self._is_cancelled:
                    break
                
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(file_items))
                batch_items = file_items[start_idx:end_idx]
                
                # å¹¶è¡Œæ‰§è¡Œå½“å‰æ‰¹æ¬¡
                tasks = [
                    loop.run_in_executor(executor, verify_single_file, item)
                    for item in batch_items
                ]
                
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # å¤„ç†ç»“æœ
                for result in batch_results:
                    if isinstance(result, Exception):
                        continue
                    
                    item, exists = result
                    if exists:
                        existing_files.append(item)
                    else:
                        files_to_download.append(item)
                
                # æ›´æ–°è¿›åº¦ï¼ˆæ”¯æŒåç§»ï¼‰
                batch_progress = ((batch_idx + 1) / total_batches) * 50  # éªŒè¯é˜¶æ®µå 50%
                total_progress = progress_offset + batch_progress
                self.check_progress.emit(total_progress)
                await asyncio.sleep(0)
        
        return existing_files, files_to_download
    
    async def download_files(self, file_items: List[FileItem], output_dir: Path, data_manager=None) -> Dict[str, bool]:
        """ä¸‹è½½å¤šä¸ªæ–‡ä»¶"""
        if not file_items:
            return {}
        
        self._is_cancelled = False
        self._is_downloading = True
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # å‘é€å¼€å§‹ä¿¡å·
        self.download_started.emit()
        self.log_message.emit(f"å¼€å§‹ä¸‹è½½ {len(file_items)} ä¸ªæ–‡ä»¶åˆ° {output_dir}")
        
        # å¼‚æ­¥é¢„å…ˆè¿‡æ»¤å·²å­˜åœ¨çš„æ–‡ä»¶ - æ•ˆç‡ä¼˜åŒ–
        results = {}
        self.log_message.emit("æ­£åœ¨æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡ä»¶...")
        
        existing_files, files_to_download = await self._batch_check_existing_files(file_items, output_dir, data_manager)
        
        # å‘é€æ£€æŸ¥å®Œæˆä¿¡å·
        self.check_progress.emit(100.0)
        
        # å¤„ç†å·²å­˜åœ¨çš„æ–‡ä»¶ - é˜¶æ®µä¸€ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†ï¼Œå‡å°‘æ—¥å¿—è¾“å‡º
        completed_count = 0
        total_count = len(file_items)
        
        if existing_files:
            self.log_message.emit(f"ğŸ“ æ‰¹é‡å¤„ç† {len(existing_files)} ä¸ªå·²å­˜åœ¨çš„æ–‡ä»¶...")
            
            # é‡ç½®è¿›åº¦æ—¥å¿—æ ‡è®°
            for attr in ['_logged_25', '_logged_50', '_logged_75']:
                if hasattr(self, attr):
                    delattr(self, attr)
            
            # åŠ¨æ€è®¡ç®—æ‰¹æ¬¡å¤§å°ï¼šæ ¹æ®æ–‡ä»¶æ•°é‡æ™ºèƒ½è°ƒæ•´
            if len(existing_files) <= 500:
                batch_size = 100  # å°é‡æ–‡ä»¶ï¼š100ä¸ªä¸€æ‰¹
            elif len(existing_files) <= 5000:
                batch_size = 1000  # ä¸­é‡æ–‡ä»¶ï¼š1000ä¸ªä¸€æ‰¹  
            elif len(existing_files) <= 20000:
                batch_size = 5000  # å¤§é‡æ–‡ä»¶ï¼š5000ä¸ªä¸€æ‰¹
            else:
                batch_size = 10000  # è¶…å¤§é‡æ–‡ä»¶ï¼š10000ä¸ªä¸€æ‰¹
            for batch_start in range(0, len(existing_files), batch_size):
                if self._is_cancelled:
                    break
                
                batch_end = min(batch_start + batch_size, len(existing_files))
                batch = existing_files[batch_start:batch_end]
                
                for item in batch:
                    item.mark_completed(output_dir / item.full_filename)
                    # é˜¶æ®µä¸€æ–°å¢ï¼šæ›´æ–°ç£ç›˜å…ƒæ•°æ®
                    item.update_disk_metadata(output_dir / item.full_filename)
                    results[item.filename] = True
                    completed_count += 1
                
                # æ‰¹é‡æ›´æ–°è¿›åº¦å’ŒUI
                overall_progress = (completed_count / total_count) * 100
                self.overall_progress.emit(overall_progress, completed_count, total_count)
                
                # æ™ºèƒ½è¿›åº¦æŠ¥å‘Šï¼šåªåœ¨é‡è¦èŠ‚ç‚¹è¾“å‡ºæ—¥å¿—
                progress_ratio = batch_end / len(existing_files)
                should_log = (
                    batch_end < len(existing_files) and (
                        progress_ratio >= 0.25 and not hasattr(self, '_logged_25') or
                        progress_ratio >= 0.50 and not hasattr(self, '_logged_50') or  
                        progress_ratio >= 0.75 and not hasattr(self, '_logged_75')
                    )
                )
                
                if should_log:
                    self.log_message.emit(f"âœ… å·²å¤„ç† {batch_end}/{len(existing_files)} ä¸ªç°æœ‰æ–‡ä»¶ ({progress_ratio:.0%})")
                    if progress_ratio >= 0.25: self._logged_25 = True
                    if progress_ratio >= 0.50: self._logged_50 = True  
                    if progress_ratio >= 0.75: self._logged_75 = True
                
                # è®©å‡ºæ§åˆ¶æƒï¼Œä¿æŒUIå“åº” - å¤§æ‰¹æ¬¡æ—¶å‡å°‘ç¡çœ é¢‘ç‡
                if batch_size >= 5000:
                    await asyncio.sleep(0.001)  # å¤§æ‰¹æ¬¡å¿«é€Ÿå¤„ç†
                else:
                    await asyncio.sleep(0.002)  # å°æ‰¹æ¬¡ç¨å¾®å¤šè®©å‡ºä¸€ç‚¹æ—¶é—´
            
            # æ±‡æ€»ä¿¡æ¯ï¼Œæ›¿ä»£é€ä¸ªæ–‡ä»¶çš„æ—¥å¿—
            self.log_message.emit(f"âœ… æ‰¹é‡è·³è¿‡ {len(existing_files)} ä¸ªå·²å­˜åœ¨æ–‡ä»¶ï¼ŒèŠ‚çœä¸‹è½½æ—¶é—´")
        
        skipped_count = len(existing_files)
        
        if skipped_count > 0:
            self.log_message.emit(f"è·³è¿‡ {skipped_count} ä¸ªå·²å­˜åœ¨çš„æ–‡ä»¶")
        
        if not files_to_download:
            self.log_message.emit("æ‰€æœ‰æ–‡ä»¶éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€ä¸‹è½½")
            # ç¡®ä¿æœ€ç»ˆè¿›åº¦ä¸º100%
            self.overall_progress.emit(100.0, completed_count, total_count)
            self.download_finished.emit(len(file_items), 0)
            self._is_downloading = False
            return results
        
        self.log_message.emit(f"éœ€è¦ä¸‹è½½ {len(files_to_download)} ä¸ªæ–‡ä»¶")
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘ - ä½¿ç”¨åŸºäºæ–‡ä»¶ç±»å‹çš„ä¼˜åŒ–å¹¶å‘æ•°
        optimal_concurrent = self.config.get_optimal_concurrent_requests(len(file_items), len(files_to_download), files_to_download)
        self.log_message.emit(f"ä½¿ç”¨æ™ºèƒ½ä¼˜åŒ–å¹¶å‘æ•°: {optimal_concurrent} (åŸºäºæ–‡ä»¶ç±»å‹å’Œå¤§å°åˆ†æ)")
        self._semaphore = asyncio.Semaphore(optimal_concurrent)
        
        # åˆ›å»ºç½‘ç»œé…ç½®å’Œå®¢æˆ·ç«¯
        self._network_config = self.config.create_network_config(files_to_download)
        
        # HTTP/2 æ”¯æŒæ£€æµ‹å’Œé™çº§
        if self.config.use_http2 and self.config.auto_detect_http2:
            try:
                http2_supported = await self.network_manager.probe_http2_support(self.config.asset_base_url)
                if http2_supported:
                    self._http2_enabled = True
                    self.log_message.emit("ğŸš€ HTTP/2 æ”¯æŒå·²å¯ç”¨ï¼Œè¿æ¥å¤ç”¨ä¼˜åŒ–æ¿€æ´»")
                else:
                    self._http2_enabled = False
                    self._network_config.use_http2 = False
                    self.log_message.emit("âš ï¸  æœåŠ¡å™¨ä¸æ”¯æŒHTTP/2ï¼Œè‡ªåŠ¨é™çº§åˆ°HTTP/1.1")
            except Exception as e:
                self._http2_enabled = False
                self._network_config.use_http2 = False
                self.log_message.emit(f"âš ï¸  HTTP/2æ£€æµ‹å¤±è´¥ï¼Œé™çº§åˆ°HTTP/1.1: {str(e)}")
        
        try:
            # ä½¿ç”¨æ–°çš„ç½‘ç»œå®¢æˆ·ç«¯
            async with AsyncHttpClient(self._network_config) as http_client:
                self._http_client = http_client
                
                # åˆ†æ‰¹å¤„ç†éœ€è¦ä¸‹è½½çš„æ–‡ä»¶ - ä½¿ç”¨åŸºäºæ–‡ä»¶ç±»å‹çš„æ™ºèƒ½æ‰¹æ¬¡å¤§å°
                optimal_batch_size = self.config.get_optimal_batch_size(len(file_items), len(files_to_download), files_to_download)
                self.log_message.emit(f"ä½¿ç”¨æ™ºèƒ½ä¼˜åŒ–æ‰¹æ¬¡å¤§å°: {optimal_batch_size} (åŸºäºæ–‡ä»¶ç±»å‹å’Œå¤§å°åˆ†æ)")
                
                batches = [
                    files_to_download[i:i + optimal_batch_size] 
                    for i in range(0, len(files_to_download), optimal_batch_size)
                ]
                
                # completed_count å·²ç»åœ¨å¤„ç†å·²å­˜åœ¨æ–‡ä»¶æ—¶åˆå§‹åŒ–äº†
                # total_count ä¹Ÿå·²ç»åˆå§‹åŒ–äº†
                
                for batch_idx, batch in enumerate(batches):
                    if self._is_cancelled:
                        break
                    
                    self.log_message.emit(f"å¤„ç†ä¸‹è½½æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}")
                    
                    # å¹¶å‘ä¸‹è½½å½“å‰æ‰¹æ¬¡
                    tasks = [
                        self._download_single_file(item, output_dir)
                        for item in batch
                    ]
                    
                    batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    # å¤„ç†æ‰¹æ¬¡ç»“æœ
                    for item, result in zip(batch, batch_results):
                        if isinstance(result, Exception):
                            item.mark_failed(str(result))
                            results[item.filename] = False
                            self.file_completed.emit(item.filename, False, str(result))
                        else:
                            results[item.filename] = result
                            if result:
                                completed_count += 1
                        
                        # æ›´æ–°æ•´ä½“è¿›åº¦
                        overall_progress = (completed_count / total_count) * 100
                        self.overall_progress.emit(overall_progress, completed_count, total_count)
                    
                    # æ‰¹æ¬¡é—´æš‚åœ - ä»…åœ¨æœ‰å®é™…ä¸‹è½½ä»»åŠ¡æ—¶æ‰æš‚åœ
                    if batch_idx < len(batches) - 1 and len(batch) > 0:
                        await asyncio.sleep(self.config.retry_delay)
                
        except Exception as e:
            self.log_message.emit(f"ä¸‹è½½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        finally:
            self._http_client = None
            self._network_config = None
            self._is_downloading = False
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for success in results.values() if success)
        failed_count = len(results) - success_count
        
        if self._is_cancelled:
            self.download_cancelled.emit()
            self.log_message.emit("ä¸‹è½½å·²å–æ¶ˆ")
        else:
            self.download_finished.emit(success_count, failed_count)
            self.log_message.emit(f"ä¸‹è½½å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        return results
    
    async def _download_single_file(self, file_item: FileItem, output_dir: Path) -> bool:
        """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
        if self._is_cancelled:
            return False
        
        async with self._semaphore:
            if self._is_cancelled:
                return False
            
            # æ„å»ºä¸‹è½½URLå’Œæœ¬åœ°è·¯å¾„
            download_url = f"{self.config.asset_base_url}/{file_item.base_filename}-{file_item.md5}{file_item.file_extension}"
            local_path = output_dir / file_item.full_filename
            
            file_item.download_url = download_url
            file_item.status = DownloadStatus.DOWNLOADING
            
            self.log_message.emit(f"å¼€å§‹ä¸‹è½½: {file_item.filename}")
            
            # äºŒæ¬¡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆé˜²æ­¢å¹¶å‘æ—¶å‡ºç°çš„ç«æ€æ¡ä»¶ï¼‰
            if local_path.exists():
                file_item.mark_completed(local_path)
                # é˜¶æ®µä¸€æ–°å¢ï¼šæ›´æ–°ç£ç›˜å…ƒæ•°æ®
                file_item.update_disk_metadata(local_path)
                self.progress_updated.emit(file_item.filename, 100.0)
                # é˜¶æ®µä¸€ä¼˜åŒ–ï¼šå‡å°‘é‡å¤çš„"å·²å­˜åœ¨"æ—¥å¿—ï¼Œä»…åœ¨UIå±‚é¢é€šçŸ¥
                self.file_completed.emit(file_item.filename, True, "æ–‡ä»¶å·²å­˜åœ¨")
                return True
            
            # é‡è¯•æœºåˆ¶
            for attempt in range(self.config.max_retries):
                if self._is_cancelled:
                    return False
                
                try:
                    return await self._download_with_progress(file_item, download_url, local_path)
                except Exception as e:
                    error_msg = f"ä¸‹è½½å¤±è´¥ (å°è¯• {attempt + 1}/{self.config.max_retries}): {str(e)}"
                    self.log_message.emit(f"{file_item.filename} - {error_msg}")
                    
                    if attempt < self.config.max_retries - 1:
                        await asyncio.sleep(self.config.retry_delay)
                    else:
                        file_item.mark_failed(error_msg)
                        self.file_completed.emit(file_item.filename, False, error_msg)
                        return False
            
            return False
    
    async def _download_with_progress(self, file_item: FileItem, url: str, local_path: Path) -> bool:
        """å¸¦è¿›åº¦çš„ä¸‹è½½ - HTTP/2 ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # ä½¿ç”¨è‡ªé€‚åº”å—å¤§å°
            adaptive_chunk_size = self.config.get_adaptive_chunk_size(file_item)
            
            # å‡†å¤‡è¯·æ±‚å¤´
            headers = {}
            if file_item.filename.endswith('.json'):
                headers['Accept-Encoding'] = 'gzip, br, deflate'
            
            # ä½¿ç”¨æ–°çš„ç½‘ç»œå®¢æˆ·ç«¯æµå¼ä¸‹è½½
            async with self._http_client.stream_download(url, headers) as response:
                if response.status_code != 200:
                    raise Exception(f"HTTP {response.status_code}")
                
                # è·å–æ–‡ä»¶å¤§å°
                if response.content_length:
                    file_item.size = response.content_length
                
                # æ£€æµ‹åè®®ç‰ˆæœ¬
                protocol_info = "HTTP/2" if self._http2_enabled else "HTTP/1.1"
                
                # ä¸‹è½½æ–‡ä»¶
                if file_item.is_binary_file:
                    # äºŒè¿›åˆ¶æ–‡ä»¶ - æµå¼ä¸‹è½½
                    async with aiofiles.open(local_path, 'wb') as f:
                        downloaded = 0
                        async for chunk in response.iter_chunks(adaptive_chunk_size):
                            if self._is_cancelled:
                                return False
                            
                            await f.write(chunk)
                            downloaded += len(chunk)
                            
                            # æ›´æ–°è¿›åº¦
                            if file_item.size:
                                progress = (downloaded / file_item.size) * 100
                                file_item.progress = progress
                                file_item.downloaded_size = downloaded
                                self.progress_updated.emit(file_item.filename, progress)
                else:
                    # æ–‡æœ¬æ–‡ä»¶ - æµå¼è¯»å–
                    content_bytes = b''
                    async for chunk in response.iter_chunks(adaptive_chunk_size):
                        if self._is_cancelled:
                            return False
                        content_bytes += chunk
                    
                    # è§£ç å¹¶å†™å…¥
                    try:
                        content = content_bytes.decode('utf-8')
                    except UnicodeDecodeError:
                        content = content_bytes.decode('utf-8', errors='replace')
                    
                    async with aiofiles.open(local_path, 'w', encoding='utf-8') as f:
                        await f.write(content)
                    
                    file_item.progress = 100.0
                    self.progress_updated.emit(file_item.filename, 100.0)
                
                # æ ‡è®°å®Œæˆ
                file_item.mark_completed(local_path)
                # é˜¶æ®µä¸€æ–°å¢ï¼šæ›´æ–°ç£ç›˜å…ƒæ•°æ®
                file_item.update_disk_metadata(local_path)
                
                # é˜¶æ®µäºŒæ–°å¢ï¼šæ›´æ–°Bloom Filter
                self._update_bloom_filter_on_completion(file_item)
                
                # è®°å½•ä¸‹è½½æ€§èƒ½ä¿¡æ¯
                if file_item.size:
                    file_type = "å¤§æ–‡ä»¶" if file_item.size > self.config.large_file_threshold else "å°æ–‡ä»¶" if file_item.size < self.config.small_file_threshold else "ä¸­ç­‰æ–‡ä»¶"
                    compression_info = "å‹ç¼©ä¼ è¾“" if 'gzip' in headers.get('Accept-Encoding', '') else "åŸå§‹ä¼ è¾“"
                    self.log_message.emit(f"âœ… {protocol_info} {file_type} {file_item.filename} ä¸‹è½½å®Œæˆ "
                                        f"({file_item.size/1024:.1f}KB, {compression_info}, å—å¤§å°:{adaptive_chunk_size/1024:.1f}KB)")
                
                self.file_completed.emit(file_item.filename, True, "ä¸‹è½½æˆåŠŸ")
                return True
                
        except asyncio.TimeoutError:
            raise Exception(f"ä¸‹è½½è¶…æ—¶")
        except Exception as e:
            raise Exception(f"ä¸‹è½½å¤±è´¥: {str(e)}")
    
    def cancel_download(self):
        """å–æ¶ˆä¸‹è½½"""
        self._is_cancelled = True
        self._is_downloading = False
        self.log_message.emit("æ­£åœ¨å–æ¶ˆä¸‹è½½...")
    
    @property
    def is_downloading(self) -> bool:
        """æ˜¯å¦æ­£åœ¨ä¸‹è½½"""
        return self._is_downloading 

    async def _optimized_full_scan(self, file_items: List[FileItem], output_dir: Path) -> tuple[List[FileItem], List[FileItem]]:
        """ä¼˜åŒ–ç‰ˆå®Œæ•´æ‰«æ - å½“ç¼“å­˜ä¸å¯é æ—¶çš„é™çº§é€‰é¡¹"""
        self.log_message.emit("ğŸ”„ æ‰§è¡Œä¼˜åŒ–ç‰ˆå®Œæ•´æ‰«æ...")
        
        loop = asyncio.get_event_loop()
        
        # æ‰§è¡Œç›®å½•æ‰«ææ„å»ºæ–‡ä»¶æ˜ å°„
        def _scan_dir(directory: Path) -> dict[str, int]:
            mapping: dict[str, int] = {}
            try:
                with os.scandir(directory) as it:
                    for entry in it:
                        if entry.is_file():
                            try:
                                size = entry.stat().st_size
                                mapping[entry.name] = size
                            except (OSError, IOError):
                                # stat å¤±è´¥æ—¶å¿½ç•¥è¯¥æ–‡ä»¶
                                pass
            except FileNotFoundError:
                # ç›®æ ‡ç›®å½•ä¸å­˜åœ¨, è§†ä½œç©ºç›®å½•
                pass
            return mapping

        self.log_message.emit("ğŸ“‚ æ‰«æç›®å½•æ„å»ºæ–‡ä»¶æ˜ å°„...")
        files_meta: dict[str, int] = await loop.run_in_executor(None, _scan_dir, output_dir)
        
        # æ ¹æ®æ‰«æç»“æœåˆ†ç±»æ–‡ä»¶å¹¶æ›´æ–°å…ƒæ•°æ®
        existing_files = []
        files_to_download = []
        total_files = len(file_items)

        for idx, item in enumerate(file_items):
            if self._is_cancelled:
                break

            size_on_disk = files_meta.get(item.full_filename)
            if size_on_disk is not None and (item.size is None or size_on_disk == item.size):
                # æ–‡ä»¶å­˜åœ¨ï¼Œæ›´æ–°å…ƒæ•°æ®
                file_path = output_dir / item.full_filename
                item.update_disk_metadata(file_path)
                item.mark_completed(file_path)
                existing_files.append(item)
            else:
                files_to_download.append(item)

            # å®šæœŸæ›´æ–°è¿›åº¦
            if idx % 200 == 0 or idx == total_files - 1:
                progress_percent = (idx / total_files) * 100
                self.check_progress.emit(progress_percent)
                await asyncio.sleep(0)

        self.log_message.emit(f"âœ… å®Œæ•´æ‰«æå®Œæˆ: å‘ç° {len(existing_files)} ä¸ªç°æœ‰æ–‡ä»¶")
        return existing_files, files_to_download 

     
    
    async def _optimized_bloom_filter_check(self, file_items: List[FileItem], output_dir: Path, 
                                           bloom_filter, data_manager) -> tuple[List[FileItem], List[FileItem]]:
        """ä¼˜åŒ–çš„Bloom Filteræ£€æŸ¥ - æœ€ä¼˜æ‰§è¡Œé¡ºåº"""
        self.log_message.emit("ğŸš€ æ‰§è¡Œæœ€ä¼˜é¡ºåºï¼šBloom Filter â†’ ç¼“å­˜åˆ†æ â†’ ç²¾ç¡®æ£€æŸ¥")
        
        # ç¬¬ä¸€é˜¶æ®µï¼šBloom Filter O(1)å¿«é€Ÿé¢„è¿‡æ»¤
        self.log_message.emit("âš¡ é˜¶æ®µ1: Bloom Filter O(1)é¢„è¿‡æ»¤å…¨éƒ¨æ–‡ä»¶...")
        likely_existing, definitely_new = bloom_filter.fast_pre_filter(file_items)
        
        filter_info = bloom_filter.get_info()
        reduction_ratio = len(definitely_new) / len(file_items) * 100
        self.log_message.emit(
            f"ğŸ“Š Bloomè¿‡æ»¤å®Œæˆ: {len(definitely_new)} ä¸ªæ–‡ä»¶ç¡®è®¤æ–°å¢ ({reduction_ratio:.1f}%), "
            f"{len(likely_existing)} ä¸ªéœ€è¦è¿›ä¸€æ­¥åˆ†æ"
        )
        
        existing_files = []
        files_to_download = list(definitely_new)  # ç¡®å®šä¸å­˜åœ¨çš„æ–‡ä»¶ç›´æ¥å½’ç±»
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¯¹å¯èƒ½å­˜åœ¨çš„æ–‡ä»¶è¿›è¡Œç¼“å­˜å¯é æ€§åˆ†æ
        if likely_existing:
            self.log_message.emit(f"ğŸ§  é˜¶æ®µ2: ç¼“å­˜åˆ†æ {len(likely_existing)} ä¸ªå¯èƒ½å­˜åœ¨çš„æ–‡ä»¶...")
            
            cache_analysis = data_manager.analyze_cache_reliability(likely_existing, output_dir)
            self.log_message.emit(f"ğŸ“Š ç¼“å­˜åˆ†æ: {cache_analysis['reason']}")
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šæ ¹æ®ç¼“å­˜å¯é æ€§é€‰æ‹©æœ€ä¼˜ç²¾ç¡®æ£€æŸ¥ç­–ç•¥
            self.log_message.emit(f"ğŸ” é˜¶æ®µ3: ç²¾ç¡®æ£€æŸ¥ç­–ç•¥é€‰æ‹©...")
            
            if cache_analysis['recommendation'] == 'cache_reliable':
                self.log_message.emit("âœ… ç¼“å­˜å¯é ï¼Œä½¿ç”¨ç¼“å­˜ä¼˜å…ˆæ£€æŸ¥")
                precise_existing, precise_new = await self._cache_based_check(likely_existing, output_dir)
            elif cache_analysis['recommendation'] == 'incremental_check':
                self.log_message.emit("ğŸ”„ ç¼“å­˜éƒ¨åˆ†å¯é ï¼Œä½¿ç”¨å¢é‡æ£€æŸ¥")
                precise_existing, precise_new = await self._smart_incremental_check(likely_existing, output_dir, cache_analysis)
            else:
                self.log_message.emit("âš ï¸  ç¼“å­˜ä¸å¯é ï¼Œä½¿ç”¨å®Œæ•´æ‰«æ")
                precise_existing, precise_new = await self._optimized_full_scan(likely_existing, output_dir)
            
            existing_files.extend(precise_existing)
            files_to_download.extend(precise_new)
        
        # ç¬¬å››é˜¶æ®µï¼šæ€»ç»“ä¼˜åŒ–æ•ˆæœ
        total_files = len(file_items)
        bloom_saved = len(definitely_new)
        cache_processed = len(likely_existing)
        efficiency = bloom_saved / total_files * 100
        
        self.log_message.emit(
            f"âœ… ä¸‰é˜¶æ®µä¼˜åŒ–å®Œæˆ: BloomèŠ‚çœ {bloom_saved} æ¬¡æ£€æŸ¥ ({efficiency:.1f}%), "
            f"ç¼“å­˜å¤„ç† {cache_processed} ä¸ªæ–‡ä»¶, "
            f"è¯¯åˆ¤ç‡ {filter_info['estimated_false_positive']:.2%}"
        )
        
        return existing_files, files_to_download
    
    def _update_bloom_filter_on_completion(self, file_item: FileItem):
        """ä¸‹è½½å®Œæˆåæ›´æ–°Bloom Filter"""
        try:
            # ä»DataManagerå¯¼å…¥ç”¨äºè·å–Bloom Filter
            from .persistence import DataManager
            
            # è¿™é‡Œæˆ‘ä»¬ä¸ç›´æ¥è®¿é—®DataManagerå®ä¾‹ï¼Œè€Œæ˜¯é€šè¿‡ä¿¡å·é€šçŸ¥æ›´æ–°
            # å®é™…çš„Bloom Filteræ›´æ–°ä¼šåœ¨UIå±‚å¤„ç†
            pass
        except Exception as e:
            # å¿½ç•¥Bloom Filteræ›´æ–°é”™è¯¯ï¼Œä¸å½±å“ä¸»æµç¨‹
            pass 