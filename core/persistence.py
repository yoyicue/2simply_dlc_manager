"""
æ•°æ®æŒä¹…åŒ–æ¨¡å—
"""
import json
from pathlib import Path
from typing import List, Dict, Optional, Any
from .models import FileItem, DownloadStatus, MD5VerifyStatus
from .utils import get_app_data_file, ensure_writable_path, is_running_from_bundle


class DataManager:
    """æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_file: Optional[Path] = None):
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼Œè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ä½ç½®
        if data_file is None:
            if is_running_from_bundle():
                # æ‰“åŒ…åº”ç”¨ï¼šä½¿ç”¨ç”¨æˆ·æ•°æ®ç›®å½•
                self.data_file = get_app_data_file("dlc_download_state.json")
            else:
                # å¼€å‘ç¯å¢ƒï¼šä¼˜å…ˆä½¿ç”¨å½“å‰ç›®å½•ï¼Œå¦‚æœä¸å¯å†™åˆ™ä½¿ç”¨ç”¨æˆ·æ•°æ®ç›®å½•
                self.data_file = ensure_writable_path(Path("dlc_download_state.json"))
        else:
            # ç”¨æˆ·æŒ‡å®šçš„è·¯å¾„ï¼šç¡®ä¿å¯å†™
            self.data_file = ensure_writable_path(data_file)
        
        # é˜¶æ®µäºŒæ–°å¢ï¼šBloom Filterç¼“å­˜
        self.bloom_filter = None
        self._bloom_enabled = True
    
    def enable_bloom_filter(self, enabled: bool = True):
        """å¯ç”¨/ç¦ç”¨Bloom Filter"""
        self._bloom_enabled = enabled
        if not enabled:
            self.bloom_filter = None
    
    def build_bloom_filter(self, file_items: List[FileItem]) -> Optional[Dict[str, Any]]:
        """æ„å»ºBloom Filterç¼“å­˜"""
        if not self._bloom_enabled:
            return None
        
        try:
            from utils.bloom_filter import FileBloomFilter
            
            # åˆ›å»ºæ–‡ä»¶ä¸“ç”¨Bloom Filter
            self.bloom_filter = FileBloomFilter(expected_files=len(file_items))
            
            # ä»å·²å®Œæˆæ–‡ä»¶æ„å»º
            build_info = self.bloom_filter.build_from_completed_files(file_items)
            
            return build_info
            
        except Exception as e:
            print(f"æ„å»ºBloom Filterå¤±è´¥: {e}")
            self.bloom_filter = None
            return None
    
    def get_bloom_filter_info(self) -> Optional[Dict[str, Any]]:
        """è·å–Bloom Filterä¿¡æ¯"""
        if self.bloom_filter and self.bloom_filter.is_cache_valid():
            return self.bloom_filter.get_info()
        return None
    
    def load_file_mapping(self, json_file: Path) -> List[FileItem]:
        """ä»BigFilesMD5s.jsonåŠ è½½æ–‡ä»¶æ˜ å°„"""
        try:
            content = json_file.read_text(encoding='utf-8')
            content = content.strip()
            
            # å¤„ç†å¯èƒ½çš„JSONæ ¼å¼é—®é¢˜
            if content.endswith(',}'):
                content = content[:-2] + '}'
            elif content.endswith(','):
                content = content[:-1]
            
            file_mapping = json.loads(content)
            
            file_items = []
            for filename, md5 in file_mapping.items():
                item = FileItem(filename=filename, md5=md5)
                file_items.append(item)
            
            return file_items
        
        except Exception as e:
            raise Exception(f"åŠ è½½æ–‡ä»¶æ˜ å°„å¤±è´¥: {str(e)}")
    
    def load_file_mapping_with_state_diff(self, json_file: Path) -> tuple[List[FileItem], Dict[str, int]]:
        """ä»BigFilesMD5s.jsonåŠ è½½æ–‡ä»¶æ˜ å°„ï¼Œå¹¶ä¸ç°æœ‰çŠ¶æ€è¿›è¡Œdiffåˆå¹¶
        
        Returns:
            (merged_file_items, diff_info)
            diff_infoåŒ…å«: {'new': count, 'existing': count, 'updated': count, 'removed': count}
        """
        try:
            # 1. åŠ è½½æ–°çš„æ–‡ä»¶æ˜ å°„
            content = json_file.read_text(encoding='utf-8')
            content = content.strip()
            
            # å¤„ç†å¯èƒ½çš„JSONæ ¼å¼é—®é¢˜
            if content.endswith(',}'):
                content = content[:-2] + '}'
            elif content.endswith(','):
                content = content[:-1]
            
            file_mapping = json.loads(content)
            
            # 2. å°è¯•åŠ è½½ç°æœ‰çŠ¶æ€
            existing_items = {}
            try:
                saved_items, _ = self.load_state()
                for item in saved_items:
                    # ä½¿ç”¨(filename, md5)ä½œä¸ºå”¯ä¸€é”®
                    key = (item.filename, item.md5)
                    existing_items[key] = item
            except:
                # å¦‚æœæ²¡æœ‰ç°æœ‰çŠ¶æ€æˆ–åŠ è½½å¤±è´¥ï¼Œç»§ç»­å¤„ç†
                pass
            
            # 3. è¿›è¡Œdiffåˆå¹¶
            merged_items = []
            diff_info = {'new': 0, 'existing': 0, 'updated': 0, 'removed': 0}
            
            # å¤„ç†æ–°æ–‡ä»¶æ˜ å°„ä¸­çš„æ¯ä¸ªæ–‡ä»¶
            for filename, md5 in file_mapping.items():
                key = (filename, md5)
                
                if key in existing_items:
                    # æ–‡ä»¶å·²å­˜åœ¨ï¼Œä¿ç•™åŸæœ‰çŠ¶æ€
                    existing_item = existing_items[key]
                    merged_items.append(existing_item)
                    diff_info['existing'] += 1
                else:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åŒåæ–‡ä»¶ä½†MD5ä¸åŒï¼ˆæ–‡ä»¶æ›´æ–°ï¼‰
                    updated_existing = False
                    for (existing_filename, existing_md5), existing_item in existing_items.items():
                        if existing_filename == filename and existing_md5 != md5:
                            # åŒåæ–‡ä»¶ä½†MD5ä¸åŒï¼Œé‡ç½®çŠ¶æ€
                            new_item = FileItem(filename=filename, md5=md5)
                            merged_items.append(new_item)
                            diff_info['updated'] += 1
                            updated_existing = True
                            break
                    
                    if not updated_existing:
                        # å…¨æ–°æ–‡ä»¶
                        new_item = FileItem(filename=filename, md5=md5)
                        merged_items.append(new_item)
                        diff_info['new'] += 1
            
            # è®¡ç®—è¢«ç§»é™¤çš„æ–‡ä»¶ï¼ˆåœ¨æ—§çŠ¶æ€ä¸­å­˜åœ¨ä½†æ–°æ˜ å°„ä¸­ä¸å­˜åœ¨ï¼‰
            new_mapping_keys = {(filename, md5) for filename, md5 in file_mapping.items()}
            for key in existing_items.keys():
                if key not in new_mapping_keys:
                    diff_info['removed'] += 1
            
            return merged_items, diff_info
            
        except Exception as e:
            raise Exception(f"åŠ è½½æ–‡ä»¶æ˜ å°„å¤±è´¥: {str(e)}")
    
    def save_state(self, file_items: List[FileItem], output_dir: Path):
        """ä¿å­˜ä¸‹è½½çŠ¶æ€ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘CPUå ç”¨"""
        try:
            from datetime import datetime
            import time
            
            start_time = time.time()
            total_files = len(file_items)
            
            # ğŸš€ CPUä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„JSONåºåˆ—åŒ–ç­–ç•¥
            state_data = {
                'output_dir': str(output_dir) if output_dir else None,
                'metadata_version': '1.0',
                'last_full_scan': datetime.now().isoformat(),
                'directory_structure': 'flat',
                'total_files': total_files,
                'files': []
            }
            
            # ğŸš€ åˆ†å—å¤„ç†ï¼šå°†å¤§æ•°æ®é›†åˆ†æˆå°å—å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§å ç”¨å¤§é‡CPU
            chunk_size = 1000  # æ¯æ¬¡å¤„ç†1000ä¸ªæ–‡ä»¶
            processed_count = 0
            
            for i in range(0, total_files, chunk_size):
                chunk_end = min(i + chunk_size, total_files)
                chunk_items = file_items[i:chunk_end]
                
                # æ‰¹é‡å¤„ç†å½“å‰å—
                chunk_data = []
                for item in chunk_items:
                    # ğŸš€ ä¼˜åŒ–ï¼šå‡å°‘å­—å…¸åˆ›å»ºå¼€é”€ï¼ŒåªåŒ…å«å¿…è¦å­—æ®µ
                    file_data = {
                        'filename': item.filename,
                        'md5': item.md5,
                        'status': item.status.value,
                        'progress': item.progress,
                        'size': item.size,
                        'downloaded_size': item.downloaded_size,
                        'local_path': str(item.local_path) if item.local_path else None,
                        'error_message': item.error_message,
                        'download_url': item.download_url,
                        'mtime': item.mtime,
                        'disk_verified': item.disk_verified,
                        'last_checked': item.last_checked,
                        'cache_version': item.cache_version,
                        'md5_verify_status': item.md5_verify_status.value,
                        'md5_verify_time': item.md5_verify_time,
                        'calculated_md5': item.calculated_md5
                    }
                    chunk_data.append(file_data)
                
                # å°†å½“å‰å—æ·»åŠ åˆ°ä¸»æ•°æ®ç»“æ„
                state_data['files'].extend(chunk_data)
                processed_count += len(chunk_items)
                
                # ğŸš€ æ¯å¤„ç†ä¸€å®šæ•°é‡çš„æ–‡ä»¶åï¼Œè®©å‡ºCPUæ—¶é—´
                if processed_count % 5000 == 0:
                    import time
                    time.sleep(0.001)  # çŸ­æš‚ä¼‘çœ ï¼Œé¿å…100% CPUå ç”¨
            
            # ğŸš€ ä½¿ç”¨æ›´é«˜æ•ˆçš„JSONå†™å…¥æ–¹å¼
            # ä¸ä½¿ç”¨indent=2æ¥å‡å°‘åºåˆ—åŒ–æ—¶é—´å’Œæ–‡ä»¶å¤§å°
            json_str = json.dumps(state_data, ensure_ascii=False, separators=(',', ':'))
            
            # å†™å…¥æ–‡ä»¶
            self.data_file.write_text(json_str, encoding='utf-8')
            
            elapsed_time = time.time() - start_time
            print(f"ğŸ’¾ çŠ¶æ€ä¿å­˜å®Œæˆ: {total_files}ä¸ªæ–‡ä»¶, è€—æ—¶{elapsed_time:.2f}ç§’")
            
        except Exception as e:
            raise Exception(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {str(e)}")

    def save_state_optimized_async(self, file_items: List[FileItem], output_dir: Path):
        """å¼‚æ­¥ä¼˜åŒ–ç‰ˆæœ¬çš„çŠ¶æ€ä¿å­˜ï¼Œè¿›ä¸€æ­¥å‡å°‘ä¸»çº¿ç¨‹é˜»å¡"""
        try:
            from datetime import datetime
            import time
            
            start_time = time.time()
            total_files = len(file_items)
            
            # åŸºç¡€å…ƒæ•°æ®
            state_data = {
                'output_dir': str(output_dir) if output_dir else None,
                'metadata_version': '1.0',
                'last_full_scan': datetime.now().isoformat(),
                'directory_structure': 'flat',
                'total_files': total_files,
                'files': []
            }
            
            # ğŸš€ è¶…å¤§æ•°æ®é›†ä¼˜åŒ–ï¼šé¢„åˆ†é…åˆ—è¡¨å¤§å°
            state_data['files'] = [None] * total_files
            
            # ğŸš€ æ‰¹é‡è½¬æ¢ï¼Œå‡å°‘é‡å¤çš„å±æ€§è®¿é—®
            for i, item in enumerate(file_items):
                state_data['files'][i] = {
                    'filename': item.filename,
                    'md5': item.md5,
                    'status': item.status.value,
                    'progress': item.progress,
                    'size': item.size,
                    'downloaded_size': item.downloaded_size,
                    'local_path': str(item.local_path) if item.local_path else None,
                    'error_message': item.error_message,
                    'download_url': item.download_url,
                    'mtime': item.mtime,
                    'disk_verified': item.disk_verified,
                    'last_checked': item.last_checked,
                    'cache_version': item.cache_version,
                    'md5_verify_status': item.md5_verify_status.value,
                    'md5_verify_time': item.md5_verify_time,
                    'calculated_md5': item.calculated_md5
                }
                
                # ğŸš€ å®šæœŸè®©å‡ºCPUï¼Œé˜²æ­¢ç•Œé¢å¡é¡¿
                if i % 2000 == 0 and i > 0:
                    time.sleep(0.001)
            
            # ğŸš€ é«˜æ€§èƒ½JSONåºåˆ—åŒ–ï¼šä¸æ ¼å¼åŒ–ï¼Œå‡å°‘CPUå ç”¨
            json_str = json.dumps(state_data, ensure_ascii=False, separators=(',', ':'))
            
            # å†™å…¥æ–‡ä»¶
            self.data_file.write_text(json_str, encoding='utf-8')
            
            elapsed_time = time.time() - start_time
            print(f"ğŸ’¾ å¼‚æ­¥çŠ¶æ€ä¿å­˜å®Œæˆ: {total_files}ä¸ªæ–‡ä»¶, è€—æ—¶{elapsed_time:.2f}ç§’")
            
        except Exception as e:
            raise Exception(f"å¼‚æ­¥ä¿å­˜çŠ¶æ€å¤±è´¥: {str(e)}")
    
    def load_state(self) -> tuple[List[FileItem], Optional[Path]]:
        """åŠ è½½ä¸‹è½½çŠ¶æ€"""
        if not self.data_file.exists():
            return [], None
        
        try:
            state_data = json.loads(self.data_file.read_text(encoding='utf-8'))
            
            # å¤„ç† output_dirï¼Œæ³¨æ„å­—ç¬¦ä¸² "None" çš„æƒ…å†µ
            output_dir_str = state_data.get('output_dir', '')
            if output_dir_str and output_dir_str != 'None':
                output_dir = Path(output_dir_str)
            else:
                output_dir = None
            file_items = []
            
            for file_data in state_data.get('files', []):
                # æŸ¥æ‰¾çŠ¶æ€æšä¸¾
                status = DownloadStatus.PENDING
                for status_enum in DownloadStatus:
                    if status_enum.value == file_data.get('status', 'å¾…ä¸‹è½½'):
                        status = status_enum
                        break
                
                item = FileItem(
                    filename=file_data['filename'],
                    md5=file_data['md5'],
                    status=status,
                    progress=file_data.get('progress', 0.0),
                    size=file_data.get('size'),
                    downloaded_size=file_data.get('downloaded_size', 0),
                    local_path=Path(file_data['local_path']) if file_data.get('local_path') else None,
                    error_message=file_data.get('error_message'),
                    download_url=file_data.get('download_url'),
                    # é˜¶æ®µä¸€æ–°å¢ï¼šåŠ è½½å…ƒæ•°æ®å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
                    mtime=file_data.get('mtime'),
                    disk_verified=file_data.get('disk_verified', False),
                    last_checked=file_data.get('last_checked'),
                    cache_version=file_data.get('cache_version', '1.0')
                )
                
                # åŠ è½½MD5éªŒè¯çŠ¶æ€ï¼ˆå‘åå…¼å®¹ï¼‰
                md5_verify_status_str = file_data.get('md5_verify_status', 'NOT_VERIFIED')
                for verify_status in MD5VerifyStatus:
                    if verify_status.value == md5_verify_status_str:
                        item.md5_verify_status = verify_status
                        break
                
                item.md5_verify_time = file_data.get('md5_verify_time')
                item.calculated_md5 = file_data.get('calculated_md5')
                
                file_items.append(item)
            
            # é˜¶æ®µäºŒæ–°å¢ï¼šè‡ªåŠ¨æ„å»ºBloom Filter
            if file_items and self._bloom_enabled:
                try:
                    bloom_info = self.build_bloom_filter(file_items)
                    if bloom_info:
                        print(f"ğŸ” Bloom Filteræ„å»ºå®Œæˆ: {bloom_info['completed_files_count']}ä¸ªæ–‡ä»¶, "
                              f"{bloom_info['memory_usage_kb']:.1f}KBå†…å­˜")
                except Exception as e:
                    print(f"Bloom Filteræ„å»ºå¤±è´¥: {e}")
            
            return file_items, output_dir
            
        except Exception as e:
            raise Exception(f"åŠ è½½çŠ¶æ€å¤±è´¥: {str(e)}")
    
    def clear_state(self):
        """æ¸…é™¤ä¿å­˜çš„çŠ¶æ€"""
        if self.data_file.exists():
            self.data_file.unlink()
    
    def get_statistics(self, file_items: List[FileItem]) -> Dict[str, int]:
        """è·å–ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯ - CPUä¼˜åŒ–ç‰ˆæœ¬"""
        stats = {
            'total': len(file_items),
            'pending': 0,
            'downloading': 0,
            'completed': 0,
            'failed': 0,
            'cancelled': 0,
            'skipped': 0,
            'verify_failed': 0
        }
        
        # ğŸš€ CPUä¼˜åŒ–ï¼šä½¿ç”¨Counterè¿›è¡Œæ‰¹é‡ç»Ÿè®¡ï¼Œæ¯”é€ä¸ªif-elifæ›´é«˜æ•ˆ
        from collections import Counter
        status_counts = Counter(item.status for item in file_items)
        
        # æ‰¹é‡æ˜ å°„çŠ¶æ€è®¡æ•°
        for status, count in status_counts.items():
            if status == DownloadStatus.PENDING:
                stats['pending'] = count
            elif status == DownloadStatus.DOWNLOADING:
                stats['downloading'] = count
            elif status == DownloadStatus.COMPLETED:
                stats['completed'] = count
            elif status == DownloadStatus.FAILED:
                stats['failed'] = count
            elif status == DownloadStatus.CANCELLED:
                stats['cancelled'] = count
            elif status == DownloadStatus.SKIPPED:
                stats['skipped'] = count
            elif status == DownloadStatus.VERIFY_FAILED:
                stats['verify_failed'] = count
        
        return stats

    def get_statistics_cached(self, file_items: List[FileItem], cache_timeout: int = 2) -> Dict[str, int]:
        """å¸¦ç¼“å­˜çš„ç»Ÿè®¡ä¿¡æ¯è·å–ï¼Œå‡å°‘é‡å¤è®¡ç®—"""
        import time
        
        # ç®€å•çš„å®ä¾‹çº§ç¼“å­˜
        current_time = time.time()
        cache_key = f"stats_{len(file_items)}"
        
        if (hasattr(self, '_stats_cache') and 
            cache_key in self._stats_cache and 
            current_time - self._stats_cache[cache_key]['timestamp'] < cache_timeout):
            return self._stats_cache[cache_key]['data']
        
        # è®¡ç®—æ–°çš„ç»Ÿè®¡ä¿¡æ¯
        stats = self.get_statistics(file_items)
        
        # ç¼“å­˜ç»“æœ
        if not hasattr(self, '_stats_cache'):
            self._stats_cache = {}
        
        self._stats_cache[cache_key] = {
            'data': stats,
            'timestamp': current_time
        }
        
        return stats
    
    def get_total_size(self, file_items: List[FileItem]) -> tuple[int, int]:
        """è·å–æ€»å¤§å°å’Œå·²ä¸‹è½½å¤§å°ï¼ˆå­—èŠ‚ï¼‰"""
        total_size = 0
        downloaded_size = 0
        
        for item in file_items:
            if item.size:
                total_size += item.size
            downloaded_size += item.downloaded_size
        
        return total_size, downloaded_size
    
    def format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size_bytes == 0:
            return "0 B"
        
        units = ['B', 'KB', 'MB', 'GB', 'TB']
        unit_index = 0
        size = float(size_bytes)
        
        while size >= 1024 and unit_index < len(units) - 1:
            size /= 1024
            unit_index += 1
        
        return f"{size:.2f} {units[unit_index]}"
    
    def filter_files(self, file_items: List[FileItem], 
                    status_filter: Optional[DownloadStatus] = None,
                    search_text: str = "") -> List[FileItem]:
        """è¿‡æ»¤æ–‡ä»¶åˆ—è¡¨"""
        filtered_items = file_items
        
        # æŒ‰çŠ¶æ€è¿‡æ»¤
        if status_filter:
            filtered_items = [item for item in filtered_items if item.status == status_filter]
        
        # æŒ‰æœç´¢æ–‡æœ¬è¿‡æ»¤
        if search_text:
            search_text = search_text.lower()
            filtered_items = [
                item for item in filtered_items 
                if search_text in item.filename.lower() or search_text in item.md5.lower()
            ]
        
        return filtered_items
    
    def get_cache_metadata(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜å…ƒæ•°æ®ä¿¡æ¯"""
        if not self.data_file.exists():
            return {}
        
        try:
            state_data = json.loads(self.data_file.read_text(encoding='utf-8'))
            return {
                'metadata_version': state_data.get('metadata_version', '0.0'),
                'last_full_scan': state_data.get('last_full_scan'),
                'directory_structure': state_data.get('directory_structure', 'flat'),
                'total_files': state_data.get('total_files', 0),
                'file_count': len(state_data.get('files', []))
            }
        except:
            return {}
    
    def analyze_cache_reliability(self, file_items: List[FileItem], output_dir: Path, 
                                sample_ratio: float = 0.05) -> Dict[str, Any]:
        """åˆ†æç¼“å­˜å¯é æ€§
        
        Args:
            file_items: æ–‡ä»¶åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            sample_ratio: æŠ½æ ·æ¯”ä¾‹ (é»˜è®¤5%)
        
        Returns:
            åˆ†æç»“æœå­—å…¸åŒ…å«å¯é æ€§è¯„åˆ†å’Œå»ºè®®
        """
        import random
        from datetime import datetime, timedelta
        
        if not file_items:
            return {
                'reliable': False, 
                'reason': 'æ— ç¼“å­˜æ•°æ®', 
                'score': 0.0,
                'recommendation': 'full_scan'
            }
        
        # ç­›é€‰å·²å®Œæˆçš„æ–‡ä»¶ç”¨äºæŠ½æ ·
        completed_items = [item for item in file_items 
                         if item.status == DownloadStatus.COMPLETED and item.disk_verified]
        
        if not completed_items:
            return {
                'reliable': False, 
                'reason': 'æ— å·²éªŒè¯çš„å®Œæˆæ–‡ä»¶', 
                'score': 0.0,
                'recommendation': 'full_scan'
            }
        
        # è®¡ç®—æŠ½æ ·å¤§å°
        sample_size = max(10, int(len(completed_items) * sample_ratio))
        sample_size = min(sample_size, len(completed_items))
        
        sample_items = random.sample(completed_items, sample_size)
        
        # æ‰§è¡ŒæŠ½æ ·éªŒè¯
        valid_count = 0
        invalid_count = 0
        outdated_count = 0
        
        for item in sample_items:
            if not item.last_checked:
                invalid_count += 1
                continue
                
            file_path = output_dir / item.full_filename
            
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not file_path.exists():
                    invalid_count += 1
                    continue
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
                if not item.is_cache_valid(file_path, max_age_hours=24):
                    outdated_count += 1
                    continue
                
                valid_count += 1
                
            except Exception:
                invalid_count += 1
        
        # è®¡ç®—å¯é æ€§è¯„åˆ†
        total_checked = len(sample_items)
        reliability_score = valid_count / total_checked if total_checked > 0 else 0.0
        
        # åˆ†æç»“æœ
        analysis = {
            'reliable': reliability_score >= 0.9,  # 90%ä»¥ä¸Šè®¤ä¸ºå¯é 
            'score': reliability_score,
            'sample_size': sample_size,
            'valid_count': valid_count,
            'invalid_count': invalid_count,
            'outdated_count': outdated_count,
            'total_completed': len(completed_items)
        }
        
        # æ·»åŠ å»ºè®®
        if reliability_score >= 0.95:
            analysis['recommendation'] = 'cache_reliable'
            analysis['reason'] = f'ç¼“å­˜é«˜åº¦å¯é  ({reliability_score:.1%})'
        elif reliability_score >= 0.8:
            analysis['recommendation'] = 'incremental_check'
            analysis['reason'] = f'ç¼“å­˜åŸºæœ¬å¯é  ({reliability_score:.1%})ï¼Œå»ºè®®å¢é‡æ£€æŸ¥'
        else:
            analysis['recommendation'] = 'full_scan'
            analysis['reason'] = f'ç¼“å­˜å¯é æ€§ä½ ({reliability_score:.1%})ï¼Œå»ºè®®å®Œæ•´æ‰«æ'
        
        return analysis 